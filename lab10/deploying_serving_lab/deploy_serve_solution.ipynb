{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33646eef-b9d6-49b9-9c6d-e6fdf63ff94b",
   "metadata": {},
   "source": [
    "# Lab 10 - Part 2 - Deploying and Serving Models\n",
    "In this lab we will experiment with deploying a model as a pipiline with Flask.\n",
    "This lab was adopted from: https://www.analyticsvidhya.com/blog/2020/04/how-to-deploy-machine-learning-model-flask/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5b650-b306-4473-9082-8684ebbdbaf0",
   "metadata": {},
   "source": [
    "We’ll work with a Twitter dataset in this section. Our aim is to detect hate speech in Tweets. For the sake of simplicity, we say a Tweet contains hate speech if it has a racist or sexist sentiment associated with it. We will create a web page that will contain a text box like this (users will be able to search for any text).\n",
    "\n",
    "### Please note that sentiment analysis is a text classification problem, if you adapt this code base for your coursework - you front-end interface will need to adapt for showing the tags obtained for the labelled sequence of tokens in the test input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970d70e-ab54-4bbc-b340-451c7e921bd3",
   "metadata": {},
   "source": [
    "Let’s start by importing some of the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "522fac12-cb95-4d7f-abde-4888f7aef226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1adbf3b-eead-4918-b7bb-9272724e25a1",
   "metadata": {},
   "source": [
    "Next, we will read the dataset and view the top rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7eecce6-df59-4b3e-94c6-dca8a47d0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/twitter_sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a070d63-9d68-4a02-a117-dc9bc189ff49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a91ee4a-2209-4286-8667-639eb8fbed54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "577df307-2063-4aac-b78c-8d479a6aa8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1235013a-4438-453d-9ece-133ba0834824",
   "metadata": {},
   "source": [
    "Now, we will divide the data into train and test using the scikit-learn train_test_split function. We will take only 20 percent of the data for testing purposes. We will stratify the data on the label column so that the distribution of the target label will be the same in both train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09488523-e3be-4f6c-b83c-5b85057662c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.2, stratify = data['label'], random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93457275-d119-4b7a-bbdf-14b0a497efba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25569, 3), (6393, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54727105-09f6-4eec-9f06-edc75e8d6aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.929837\n",
       "1    0.070163\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65aebeb3-a684-4fce-927d-6417751f6d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.929923\n",
       "1    0.070077\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e6b556-668c-4288-ad1a-fc71a8696c61",
   "metadata": {},
   "source": [
    "Now, we will create a TF-IDF vector of the tweet column using the TfidfVectorizer and we will pass the parameter lowercase as True so that it will first convert text to lowercase. We will also keep max features as 1000 and pass the predefined list of stop words present in the scikit-learn library.\n",
    "\n",
    "First, create the object of the TFidfVectorizer, build your model and fit the model with the training data tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9fdc580-8c1a-46a6-8fb9-ca0146e1a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase= True, max_features=1000, stop_words=ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "252440ba-d594-4590-aebf-45e6dd99c578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=1000,\n",
       "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                      'afterwards', 'again', 'against', 'all',\n",
       "                                      'almost', 'alone', 'along', 'already',\n",
       "                                      'also', 'although', 'always', 'am',\n",
       "                                      'among', 'amongst', 'amoungst', 'amount',\n",
       "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
       "                                      'anyone', 'anything', 'anyway',\n",
       "                                      'anywhere', ...}))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.fit(train.tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8810d-745e-4214-88e8-e7251bd7d362",
   "metadata": {},
   "source": [
    "Use the model and transform the train and test data tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c8430b0-6e8b-43aa-960f-017a1af30684",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idf = tfidf_vectorizer.transform(train.tweet)\n",
    "test_idf  = tfidf_vectorizer.transform(test.tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dae093-fe67-44f1-855a-0bf664255430",
   "metadata": {},
   "source": [
    "Now, we will create an object of the Logistic Regression model.\n",
    "\n",
    "Remember – our focus is not on building a very accurate classification model but instead to see how we can deploy this predictive model to get the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05fb7b28-8be5-4d12-a7d9-af93aa538625",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01a1f48d-f68a-48ee-b7ea-2a7174f134f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR.fit(train_idf, train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22fe0610-7bfa-493b-a978-7c88ae91e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = model_LR.predict(train_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eedd67c-e5f9-4fb6-acd4-4ddbeec68e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = model_LR.predict(test_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c01a1255-0b01-46e7-81cb-02f23953c52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4888178913738019"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f1 score on train data\n",
    "f1_score(y_true= train.label, y_pred= predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cfe6f4c-340e-4746-916d-ad89e66bcb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45751633986928114"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true= test.label, y_pred= predict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39932306-e01d-4ea2-bc6b-f185b64c1167",
   "metadata": {},
   "source": [
    "Let’s define the steps of the pipeline:\n",
    "\n",
    "Step 1: Create a TF-IDF vector of the tweet text with 1000 features as defined above\n",
    "\n",
    "Step 2: Use a logistic regression model to predict the target labels\n",
    "\n",
    "When we use the fit() function with a pipeline object, both steps are executed. Post the model training process, we use the predict() function that uses the trained model to generate the predictions.\n",
    "\n",
    "Read more about sci-kit learn pipelines in this comprehensive article: [Build your first Machine Learning pipeline using scikit-learn](https://www.analyticsvidhya.com/blog/2020/01/build-your-first-machine-learning-pipeline-using-scikit-learn/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7efae72-5644-4c24-8924-c7a1464ee290",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps= [('tfidf', TfidfVectorizer(lowercase=True,\n",
    "                                                      max_features=1000,\n",
    "                                                      stop_words= ENGLISH_STOP_WORDS)),\n",
    "                            ('model', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ddb5b09-92aa-4ebb-88e3-e5d713869d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_features=1000,\n",
       "                                 stop_words=frozenset({'a', 'about', 'above',\n",
       "                                                       'across', 'after',\n",
       "                                                       'afterwards', 'again',\n",
       "                                                       'against', 'all',\n",
       "                                                       'almost', 'alone',\n",
       "                                                       'along', 'already',\n",
       "                                                       'also', 'although',\n",
       "                                                       'always', 'am', 'among',\n",
       "                                                       'amongst', 'amoungst',\n",
       "                                                       'amount', 'an', 'and',\n",
       "                                                       'another', 'any',\n",
       "                                                       'anyhow', 'anyone',\n",
       "                                                       'anything', 'anyway',\n",
       "                                                       'anywhere', ...}))),\n",
       "                ('model', LogisticRegression())])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(train.tweet, train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9333fb7c-b421-498d-89ad-2acb06993994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(train.tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9bd052-def9-4f6b-824a-46ea3c30b2a2",
   "metadata": {},
   "source": [
    "Now, we will test the pipeline with a sample tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c54617b1-9c06-49c6-a6e4-5f605166443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Virat Kohli, AB de Villiers set to auction their 'Green Day' kits from 2016 IPL match to raise funds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f754c25-73be-4d99-9d3f-5592cf2b6251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4614e82-f6ad-454b-a450-6fb3fd0e63c5",
   "metadata": {},
   "source": [
    "We have successfully built the machine learning pipeline and we will save this pipeline object using the dump function in the joblib library. You just need to pass the pipeline object and the file name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21cdd3f4-cfb8-4151-a8a4-45425ebc2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef3fa852-5659-4116-8d9b-b01ed74f994c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_classification.joblib']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(pipeline, filename=\"text_classification.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1487af-ebed-4813-a2d8-fdecb67aa8b7",
   "metadata": {},
   "source": [
    "It will create a file name “text_classification.joblib“. Now, we will open another Python file and use the load function of the joblib library to load the pipeline model.\n",
    "\n",
    "Let’s see how to use the saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f34843b3-3c1f-4fdd-b8f1-bc43118c19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5fd3268-2651-4fd3-b572-63f820f6db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Virat Kohli, AB de Villiers set to auction their 'Green Day' kits from 2016 IPL match to raise funds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d25a2ac-d834-4e6c-8286-2334789a43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = load(\"text_classification.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575af5b0-a528-4022-a9d9-0c25aacf05fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d685326-a676-41be-819b-4b496a1dc441",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "330ef505-51ce-4775-a49c-4aaa18b4159f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #cnn calls #michigan middle school 'buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>no comment!  in #australia   #opkillingbay #se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>it's unbelievable that in the 21st century we'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31934</th>\n",
       "      <td>31935</td>\n",
       "      <td>1</td>\n",
       "      <td>lady banned from kentucky mall. @user  #jcpenn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31946</th>\n",
       "      <td>31947</td>\n",
       "      <td>1</td>\n",
       "      <td>@user omfg i'm offended! i'm a  mailbox and i'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31947</th>\n",
       "      <td>31948</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user you don't have the balls to hashta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31948</th>\n",
       "      <td>31949</td>\n",
       "      <td>1</td>\n",
       "      <td>makes you ask yourself, who am i? then am i a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2242 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet\n",
       "13        14      1  @user #cnn calls #michigan middle school 'buil...\n",
       "14        15      1  no comment!  in #australia   #opkillingbay #se...\n",
       "17        18      1                             retweet if you agree! \n",
       "23        24      1    @user @user lumpy says i am a . prove it lumpy.\n",
       "34        35      1  it's unbelievable that in the 21st century we'...\n",
       "...      ...    ...                                                ...\n",
       "31934  31935      1  lady banned from kentucky mall. @user  #jcpenn...\n",
       "31946  31947      1  @user omfg i'm offended! i'm a  mailbox and i'...\n",
       "31947  31948      1  @user @user you don't have the balls to hashta...\n",
       "31948  31949      1   makes you ask yourself, who am i? then am i a...\n",
       "31960  31961      1  @user #sikh #temple vandalised in in #calgary,...\n",
       "\n",
       "[2242 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.label == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a35bd5-74ef-46d3-a268-b6a1f9e92839",
   "metadata": {},
   "source": [
    "Its now time to run the pipeline (i.e. data featurisation and model prediction) and make calls from a web page!\n",
    "\n",
    "The following command will start the flask app as a python command... but ideally you would run this from a command line, not from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7150943a-c1e0-43b1-941c-877f0e4a0581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"get_sentiment\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n",
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      " * Restarting with stat\n",
      " * Debugger is active!\n",
      " * Debugger PIN: 558-492-253\n",
      "127.0.0.1 - - [06/May/2021 13:55:44] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [06/May/2021 13:55:48] \"\u001b[32mPOST / HTTP/1.1\u001b[0m\" 302 -\n",
      "it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia \n",
      "product of the day: happy man #wine tool  who's   it's the #weekend? time to open up &amp; drink up!\n",
      "@user i'm not interested in a #linguistics that doesn't address #race &amp; . racism is about #power. #raciolinguistics brings√¢¬Ä¬¶\n",
      "@user why not @user mocked obama for being black.  @user @user @user @user #brexit\n",
      "yeah! new buttons in the mail for me √∞¬ü¬í¬ñ  they are so pretty! :) #jewelrymaking #buttons\n",
      "127.0.0.1 - - [06/May/2021 13:55:48] \"\u001b[37mGET /success/test HTTP/1.1\u001b[0m\" 200 -\n",
      "it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia \n",
      "product of the day: happy man #wine tool  who's   it's the #weekend? time to open up &amp; drink up!\n",
      "@user i'm not interested in a #linguistics that doesn't address #race &amp; . racism is about #power. #raciolinguistics brings√¢¬Ä¬¶\n",
      "@user why not @user mocked obama for being black.  @user @user @user @user #brexit\n",
      "yeah! new buttons in the mail for me √∞¬ü¬í¬ñ  they are so pretty! :) #jewelrymaking #buttons\n",
      "127.0.0.1 - - [06/May/2021 13:56:03] \"\u001b[37mGET /success/test HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [06/May/2021 13:56:18] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [06/May/2021 13:56:23] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [06/May/2021 13:56:28] \"\u001b[32mPOST / HTTP/1.1\u001b[0m\" 302 -\n",
      "it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia \n",
      "product of the day: happy man #wine tool  who's   it's the #weekend? time to open up &amp; drink up!\n",
      "@user i'm not interested in a #linguistics that doesn't address #race &amp; . racism is about #power. #raciolinguistics brings√¢¬Ä¬¶\n",
      "@user why not @user mocked obama for being black.  @user @user @user @user #brexit\n",
      "yeah! new buttons in the mail for me √∞¬ü¬í¬ñ  they are so pretty! :) #jewelrymaking #buttons\n",
      "127.0.0.1 - - [06/May/2021 13:56:28] \"\u001b[37mGET /success/test HTTP/1.1\u001b[0m\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python get_sentiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05453ef-800d-4705-93d3-f163602d0a29",
   "metadata": {},
   "source": [
    "Now that this is running go to  http://127.0.0.1:5000 or http://localhost:5000 and try it out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2463b8-18be-4003-a91d-99e10603eb4c",
   "metadata": {},
   "source": [
    "To stop the process just interrupt the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d2f28-766c-4e8d-a488-0165c50875e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
