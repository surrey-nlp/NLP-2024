{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33646eef-b9d6-49b9-9c6d-e6fdf63ff94b",
   "metadata": {},
   "source": [
    "# Lab 10 - Part 2 - Deploying and Serving Models\n",
    "In this lab we will experiment with deploying a model as a pipiline with Flask.\n",
    "This lab was adopted from: https://www.analyticsvidhya.com/blog/2020/04/how-to-deploy-machine-learning-model-flask/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5b650-b306-4473-9082-8684ebbdbaf0",
   "metadata": {},
   "source": [
    "We’ll work with a Twitter dataset in this section. Our aim is to detect hate speech in Tweets. For the sake of simplicity, we say a Tweet contains hate speech if it has a racist or sexist sentiment associated with it. We will create a web page that will contain a text box like this (users will be able to search for any text).\n",
    "\n",
    "### Please note that sentiment analysis is a text classification problem, if you adapt this code base for your coursework - you front-end interface will need to adapt for showing the tags obtained for the labelled sequence of tokens in the test input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970d70e-ab54-4bbc-b340-451c7e921bd3",
   "metadata": {},
   "source": [
    "Let’s start by importing some of the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522fac12-cb95-4d7f-abde-4888f7aef226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1adbf3b-eead-4918-b7bb-9272724e25a1",
   "metadata": {},
   "source": [
    "Next, we will read the dataset and view the top rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eecce6-df59-4b3e-94c6-dca8a47d0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/twitter_sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a070d63-9d68-4a02-a117-dc9bc189ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9ad1b",
   "metadata": {},
   "source": [
    "## Challenge 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91ee4a-2209-4286-8667-639eb8fbed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the shape of imported dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577df307-2063-4aac-b78c-8d479a6aa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1235013a-4438-453d-9ece-133ba0834824",
   "metadata": {},
   "source": [
    "Now, we will divide the data into train and test using the scikit-learn train_test_split function. We will take only 20 percent of the data for testing purposes. We will stratify the data on the label column so that the distribution of the target label will be the same in both train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09488523-e3be-4f6c-b83c-5b85057662c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide the dataset into train and test set.\n",
    "train, test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93457275-d119-4b7a-bbdf-14b0a497efba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54727105-09f6-4eec-9f06-edc75e8d6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aebeb3-a684-4fce-927d-6417751f6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0917d74b",
   "metadata": {},
   "source": [
    "## Challenge 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e6b556-668c-4288-ad1a-fc71a8696c61",
   "metadata": {},
   "source": [
    "Now, we will create a TF-IDF vector of the tweet column using the TfidfVectorizer and we will pass the parameter lowercase as True so that it will first convert text to lowercase. We will also keep max features as 1000 and pass the predefined list of stop words present in the scikit-learn library.\n",
    "\n",
    "First, create the object of the TFidfVectorizer, build your model and fit the model with the training data tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fdc580-8c1a-46a6-8fb9-ca0146e1a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase= True, max_features=1000, stop_words=ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252440ba-d594-4590-aebf-45e6dd99c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit the TF-IDF Vectorizer on the training data tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8810d-745e-4214-88e8-e7251bd7d362",
   "metadata": {},
   "source": [
    "Use the model and transform the train and test data tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8430b0-6e8b-43aa-960f-017a1af30684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Transform the train and test data tweets.\n",
    "train_idf = ...\n",
    "test_idf  = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c34f5",
   "metadata": {},
   "source": [
    "## Challenge 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dae093-fe67-44f1-855a-0bf664255430",
   "metadata": {},
   "source": [
    "Now, we will create an object of the Logistic Regression model.\n",
    "\n",
    "Remember – our focus is not on building a very accurate classification model but instead to see how we can deploy this predictive model to get the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb7b28-8be5-4d12-a7d9-af93aa538625",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1f48d-f68a-48ee-b7ea-2a7174f134f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit LR model on train_df and pass training labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe0610-7bfa-493b-a978-7c88ae91e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict train labels\n",
    "predict_train = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eedd67c-e5f9-4fb6-acd4-4ddbeec68e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict test labels\n",
    "predict_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a1255-0b01-46e7-81cb-02f23953c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score on train data\n",
    "f1_score(y_true = train.label, y_pred = predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe6f4c-340e-4746-916d-ad89e66bcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score on test data\n",
    "f1_score(y_true= test.label, y_pred= predict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39932306-e01d-4ea2-bc6b-f185b64c1167",
   "metadata": {},
   "source": [
    "Let’s define the steps of the pipeline:\n",
    "\n",
    "Step 1: Create a TF-IDF vector of the tweet text with 1000 features as defined above\n",
    "\n",
    "Step 2: Use a logistic regression model to predict the target labels\n",
    "\n",
    "When we use the fit() function with a pipeline object, both steps are executed. Post the model training process, we use the predict() function that uses the trained model to generate the predictions.\n",
    "\n",
    "Read more about sci-kit learn pipelines in this comprehensive article: [Build your first Machine Learning pipeline using scikit-learn](https://www.analyticsvidhya.com/blog/2020/01/build-your-first-machine-learning-pipeline-using-scikit-learn/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efae72-5644-4c24-8924-c7a1464ee290",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps= [('tfidf', TfidfVectorizer(lowercase=True,\n",
    "                                                      max_features=1000,\n",
    "                                                      stop_words= ENGLISH_STOP_WORDS)),\n",
    "                            ('model', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb5b09-92aa-4ebb-88e3-e5d713869d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(train.tweet, train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9333fb7c-b421-498d-89ad-2acb06993994",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict(train.tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9bd052-def9-4f6b-824a-46ea3c30b2a2",
   "metadata": {},
   "source": [
    "Now, we will test the pipeline with a sample tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54617b1-9c06-49c6-a6e4-5f605166443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Virat Kohli, AB de Villiers set to auction their 'Green Day' kits from 2016 IPL match to raise funds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f754c25-73be-4d99-9d3f-5592cf2b6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4614e82-f6ad-454b-a450-6fb3fd0e63c5",
   "metadata": {},
   "source": [
    "We have successfully built the machine learning pipeline and we will save this pipeline object using the dump function in the joblib library. You just need to pass the pipeline object and the file name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cdd3f4-cfb8-4151-a8a4-45425ebc2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fa852-5659-4116-8d9b-b01ed74f994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(pipeline, filename=\"text_classification.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1487af-ebed-4813-a2d8-fdecb67aa8b7",
   "metadata": {},
   "source": [
    "It will create a file name “text_classification.joblib“. Now, we will open another Python file and use the load function of the joblib library to load the pipeline model.\n",
    "\n",
    "Let’s see how to use the saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34843b3-3c1f-4fdd-b8f1-bc43118c19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd3268-2651-4fd3-b572-63f820f6db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Virat Kohli, AB de Villiers set to auction their 'Green Day' kits from 2016 IPL match to raise funds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d25a2ac-d834-4e6c-8286-2334789a43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = load(\"text_classification.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575af5b0-a528-4022-a9d9-0c25aacf05fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a35bd5-74ef-46d3-a268-b6a1f9e92839",
   "metadata": {},
   "source": [
    "Its now time to run the pipeline (i.e. data featurisation and model prediction) and make calls from a web page!\n",
    "\n",
    "The following command will start the flask app as a python command... but ideally you would run this from a command line, not from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150943a-c1e0-43b1-941c-877f0e4a0581",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python get_sentiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05453ef-800d-4705-93d3-f163602d0a29",
   "metadata": {},
   "source": [
    "Now that this is running go to  http://127.0.0.1:5000 or http://localhost:5000 and try it out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2463b8-18be-4003-a91d-99e10603eb4c",
   "metadata": {},
   "source": [
    "#### To stop the process just interrupt the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390af58e",
   "metadata": {},
   "source": [
    "### Alternates for Flask: \n",
    "[Streamlit](https://streamlit.io/)\n",
    "\n",
    "[Sample Code - Git Repo](https://github.com/alphagov/govuk-datalabs-streamlit-NER)\n",
    "\n",
    "[Sample Code - TDS tutorial](https://towardsdatascience.com/build-a-named-entity-recognition-app-with-streamlit-f157672f867f)\n",
    "\n",
    "or \n",
    "\n",
    "[Mercury](https://runmercury.com/)\n",
    "\n",
    "[Sample Project](https://towardsdatascience.com/build-elegant-web-apps-right-from-jupyter-notebook-with-mercury-78d9ebcbbcaf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
