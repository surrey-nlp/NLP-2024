{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06 - BERT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.1-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (2.2.0)\n",
      "Collecting requests>=2.19.0 (from datasets)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (4.66.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (23.2)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->datasets)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "   ---------------------------------------- 0.0/510.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 510.5/510.5 kB 10.6 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB ? eta 0:00:00\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "   ---------------------------------------- 0.0/170.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 170.9/170.9 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.3-cp310-cp310-win_amd64.whl (365 kB)\n",
      "   ---------------------------------------- 0.0/365.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 365.2/365.2 kB 11.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "   ---------------------------------------- 0.0/346.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 346.4/346.4 kB 21.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow-15.0.1-cp310-cp310-win_amd64.whl (24.8 MB)\n",
      "   ---------------------------------------- 0.0/24.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.5/24.8 MB 31.6 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 2.9/24.8 MB 30.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 4.0/24.8 MB 28.8 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 5.3/24.8 MB 28.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.4/24.8 MB 27.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 7.5/24.8 MB 26.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 9.2/24.8 MB 26.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.5/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.7/24.8 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.8/24.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 14.1/24.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.3/24.8 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.5/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.7/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.7/24.8 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.2/24.8 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.6/24.8 MB 11.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.0/24.8 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.3/24.8 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.7/24.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.2/24.8 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.8/24.8 MB 9.9 MB/s eta 0:00:00\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB ? eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 163.8/163.8 kB ? eta 0:00:00\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.4/50.4 kB 2.5 MB/s eta 0:00:00\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/121.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 121.1/121.1 kB ? eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.4/76.4 kB ? eta 0:00:00\n",
      "Installing collected packages: xxhash, urllib3, pyyaml, pyarrow-hotfix, pyarrow, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 datasets-2.18.0 dill-0.3.8 filelock-3.13.1 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.21.4 idna-3.6 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.1 pyarrow-hotfix-0.6 pyyaml-6.0.1 requests-2.31.0 urllib3-2.2.1 xxhash-3.4.1 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "     ---------------------------------------- 0.0/130.7 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/130.7 kB ? eta -:--:--\n",
      "     ----------- ------------------------- 41.0/130.7 kB 487.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 130.7/130.7 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.2-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/8.5 MB 11.2 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.9/8.5 MB 11.4 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.6/8.5 MB 12.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.4/8.5 MB 13.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.4/8.5 MB 15.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.4/8.5 MB 16.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.6/8.5 MB 17.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.5/8.5 MB 17.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.8/8.5 MB 18.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.5/8.5 MB 18.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.5/8.5 MB 17.6 MB/s eta 0:00:00\n",
      "Using cached safetensors-0.4.2-cp310-none-win_amd64.whl (269 kB)\n",
      "Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.1/2.2 MB 23.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 28.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 23.5 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "Successfully installed safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "     ---------------------------------------- 0.0/85.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 85.1/85.1 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting jinja2 (from spacy)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.16.3-cp310-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.7/12.1 MB 15.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.1/12.1 MB 21.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.2/12.1 MB 23.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.5/12.1 MB 24.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.6/12.1 MB 23.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.9/12.1 MB 24.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.1/12.1 MB 24.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/12.1 MB 25.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.1 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 26.2 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.6/181.6 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.2/122.2 kB ? eta 0:00:00\n",
      "Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "   ---------------------------------------- 0.0/394.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 394.9/394.9 kB 24.0 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.16.3-cp310-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.9/1.9 MB 30.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 29.7 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB ? eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 481.9/481.9 kB 29.5 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 28.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 23.4 MB/s eta 0:00:00\n",
      "Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.1/6.6 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.7/6.6 MB 28.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.7/6.6 MB 29.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.0/6.6 MB 28.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 30.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 28.2 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB ? eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, MarkupSafe, langcodes, cloudpathlib, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, jinja2, confection, weasel, thinc, spacy\n",
      "Successfully installed MarkupSafe-2.1.5 annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 jinja2-3.1.3 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.4 pydantic-core-2.16.3 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.1-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch) (4.9.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.1-cp310-cp310-win_amd64.whl (198.6 MB)\n",
      "   ---------------------------------------- 0.0/198.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/198.6 MB 9.6 MB/s eta 0:00:21\n",
      "   ---------------------------------------- 1.6/198.6 MB 19.8 MB/s eta 0:00:10\n",
      "    --------------------------------------- 2.9/198.6 MB 23.5 MB/s eta 0:00:09\n",
      "    --------------------------------------- 4.0/198.6 MB 23.2 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 5.8/198.6 MB 26.4 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 6.7/198.6 MB 26.8 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 8.1/198.6 MB 27.1 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 9.8/198.6 MB 27.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 11.2/198.6 MB 29.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 12.7/198.6 MB 31.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 14.0/198.6 MB 29.8 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 15.5/198.6 MB 29.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 16.8/198.6 MB 29.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 18.2/198.6 MB 29.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 19.1/198.6 MB 29.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 19.9/198.6 MB 27.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 21.1/198.6 MB 26.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 22.6/198.6 MB 26.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 23.7/198.6 MB 27.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 25.1/198.6 MB 27.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 26.8/198.6 MB 27.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 28.2/198.6 MB 26.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 30.1/198.6 MB 29.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 31.4/198.6 MB 31.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 32.9/198.6 MB 31.1 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 34.0/198.6 MB 31.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 35.0/198.6 MB 29.7 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 36.3/198.6 MB 29.7 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 37.7/198.6 MB 28.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 39.0/198.6 MB 28.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 40.2/198.6 MB 28.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 42.5/198.6 MB 28.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 44.3/198.6 MB 29.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 45.9/198.6 MB 29.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 47.3/198.6 MB 29.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 48.6/198.6 MB 31.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 50.1/198.6 MB 31.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 51.4/198.6 MB 32.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 52.5/198.6 MB 31.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 53.7/198.6 MB 28.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 55.5/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 56.9/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 58.0/198.6 MB 28.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 59.3/198.6 MB 28.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 61.1/198.6 MB 28.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 62.6/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 64.0/198.6 MB 32.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 65.3/198.6 MB 31.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 66.8/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 68.1/198.6 MB 31.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 69.5/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 70.8/198.6 MB 29.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 72.3/198.6 MB 29.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 73.7/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 75.2/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 76.0/198.6 MB 31.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 77.5/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 79.4/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 80.8/198.6 MB 29.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 81.8/198.6 MB 28.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 82.9/198.6 MB 28.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 83.9/198.6 MB 27.3 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 84.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.9/198.6 MB 4.1 MB/s eta 0:00:28\n",
      "   ----------------- ---------------------- 86.0/198.6 MB 4.0 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 86.1/198.6 MB 3.9 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 86.3/198.6 MB 3.9 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 86.5/198.6 MB 3.9 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 86.7/198.6 MB 3.8 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 87.1/198.6 MB 3.8 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 87.5/198.6 MB 3.7 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 88.1/198.6 MB 3.7 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 88.8/198.6 MB 3.6 MB/s eta 0:00:31\n",
      "   ------------------ --------------------- 89.5/198.6 MB 3.6 MB/s eta 0:00:31\n",
      "   ------------------ --------------------- 89.9/198.6 MB 3.6 MB/s eta 0:00:31\n",
      "   ------------------ --------------------- 90.0/198.6 MB 3.5 MB/s eta 0:00:31\n",
      "   ------------------ --------------------- 90.3/198.6 MB 3.5 MB/s eta 0:00:32\n",
      "   ------------------ --------------------- 91.1/198.6 MB 3.5 MB/s eta 0:00:32\n",
      "   ------------------ --------------------- 92.0/198.6 MB 3.5 MB/s eta 0:00:31\n",
      "   ------------------ --------------------- 93.0/198.6 MB 3.5 MB/s eta 0:00:31\n",
      "   ------------------ --------------------- 93.9/198.6 MB 3.4 MB/s eta 0:00:31\n",
      "   ------------------- -------------------- 94.9/198.6 MB 3.4 MB/s eta 0:00:31\n",
      "   ------------------- -------------------- 96.1/198.6 MB 3.4 MB/s eta 0:00:30\n",
      "   ------------------- -------------------- 97.2/198.6 MB 16.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 98.4/198.6 MB 17.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 99.5/198.6 MB 18.2 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 100.6/198.6 MB 22.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 102.3/198.6 MB 24.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 103.8/198.6 MB 25.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 105.0/198.6 MB 26.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 106.3/198.6 MB 27.3 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 107.6/198.6 MB 26.2 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 108.9/198.6 MB 27.3 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 110.2/198.6 MB 27.3 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 111.5/198.6 MB 28.5 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 112.8/198.6 MB 28.5 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 114.1/198.6 MB 28.5 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 115.5/198.6 MB 28.5 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 116.9/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 118.3/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 119.6/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 121.0/198.6 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 122.4/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 123.8/198.6 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 125.3/198.6 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 126.7/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 128.1/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 129.1/198.6 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 131.1/198.6 MB 29.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 132.5/198.6 MB 29.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 134.0/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 135.3/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 136.8/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 137.8/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 139.6/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 141.1/198.6 MB 32.7 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 142.1/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 144.1/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 145.5/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 146.9/198.6 MB 29.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 148.3/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 149.7/198.6 MB 29.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 151.1/198.6 MB 29.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 152.0/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 152.0/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 155.3/198.6 MB 29.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 156.7/198.6 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 158.0/198.6 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 159.4/198.6 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 160.8/198.6 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 162.3/198.6 MB 38.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 163.7/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 165.1/198.6 MB 29.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 166.5/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 167.8/198.6 MB 31.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 169.2/198.6 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 170.4/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 171.7/198.6 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 172.7/198.6 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 174.5/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 175.9/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 177.3/198.6 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 178.7/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 180.2/198.6 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.5/198.6 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 181.9/198.6 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 183.1/198.6 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 184.5/198.6 MB 9.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 185.7/198.6 MB 9.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 187.1/198.6 MB 9.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 188.5/198.6 MB 9.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 190.0/198.6 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 191.3/198.6 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 192.7/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  193.8/198.6 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.4/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.4/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.4/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.4/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.4/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.4/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.4/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.4/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.5/198.6 MB 13.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  196.3/198.6 MB 13.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  197.5/198.6 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 198.6/198.6 MB 9.9 MB/s eta 0:00:00\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting spacy-transformers\n",
      "  Downloading spacy_transformers-1.3.4-cp310-cp310-win_amd64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.5.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy-transformers) (3.7.4)\n",
      "Collecting transformers<4.37.0,>=3.4.0 (from spacy-transformers)\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "     ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/126.8 kB ? eta -:--:--\n",
      "     ----------- ------------------------- 41.0/126.8 kB 653.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 126.8/126.8 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy-transformers) (2.2.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy-transformers) (2.4.8)\n",
      "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers)\n",
      "  Downloading spacy_alignments-0.9.1-cp310-cp310-win_amd64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy-transformers) (1.26.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (3.2.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.21.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from sympy->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
      "Downloading spacy_transformers-1.3.4-cp310-cp310-win_amd64.whl (343 kB)\n",
      "   ---------------------------------------- 0.0/343.3 kB ? eta -:--:--\n",
      "   --------------------------------------  337.9/343.3 kB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  337.9/343.3 kB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 343.3/343.3 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading spacy_alignments-0.9.1-cp310-cp310-win_amd64.whl (187 kB)\n",
      "   ---------------------------------------- 0.0/187.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 187.7/187.7 kB 11.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "   ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.7/8.2 MB 21.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.6/8.2 MB 21.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.4/8.2 MB 21.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.4/8.2 MB 21.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.2/8.2 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.6/8.2 MB 22.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.7/8.2 MB 21.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.8/8.2 MB 21.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.2/8.2 MB 20.2 MB/s eta 0:00:00\n",
      "Installing collected packages: spacy-alignments, transformers, spacy-transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.2\n",
      "    Uninstalling transformers-4.38.2:\n",
      "      Successfully uninstalled transformers-4.38.2\n",
      "Successfully installed spacy-alignments-0.9.1 spacy-transformers-1.3.4 transformers-4.36.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'was', 'developed', '.']\n",
      "['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O']\n"
     ]
    }
   ],
   "source": [
    "for i in dataset[\"train\"]:\n",
    "    print(i['tokens'])\n",
    "    print(i['ner_tags'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertTokenizer, BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", )\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_dataset = dataset[\"train\"][:100]\n",
    "# short_dataset[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'for', 'this', 'purpose', 'the', 'gothenburg', 'young', 'persons', 'empowerment', 'scale', '(', 'g', '##ype', '##s', ')', 'was', 'developed', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(short_dataset[\"tokens\"], is_split_into_words=True)\n",
    "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "for token in tokenized_input[\"input_ids\"]:\n",
    "    print(tokenizer.convert_ids_to_tokens(token))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0], [1, 0, 2, 3, 3, 0], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 0, 1, 0, 0], [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 3, 0, 1, 0, 0], [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 3, 3, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 3, 3, 0, 1, 0, 2, 3, 3, 3, 3, 3, 0, 1, 0, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 2, 3, 0, 1, 0, 2, 3, 3, 0, 1, 0, 2, 3, 0, 1, 0, 2, 3, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 2, 3, 1, 3, 3, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 1, 3, 3, 3, 3, 3, 0, 1, 0, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 3, 0, 0], [2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [2, 3, 3, 0, 1, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 2, 3, 3, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 2, 0, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 1, 0, 2, 3, 3, 3, 0, 1, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 2, 3, 1, 3, 0], [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 0, 0, 0, 0, 0], [1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 2, 3, 3, 0, 1, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 0, 1, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 2, 3, 0, 1, 0, 2, 3, 3, 0], [0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 2, 3, 3, 3, 0, 1, 0, 2, 3, 3, 3, 0, 1, 0, 2, 3, 3, 3, 0, 1, 0, 2, 3, 3, 3, 0], [2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 2, 3, 3, 3, 3, 3, 3, 0, 1, 0, 2, 0, 1, 0, 2, 3, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 3, 3, 3, 3, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 2, 3, 3, 0, 1, 0, 0, 2, 3, 0, 1, 0, 0, 2, 3, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 2, 3, 3, 0, 1, 0, 2, 3, 3, 0, 1, 0, 2, 3, 3, 0], [2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "label_list = []\n",
    "\n",
    "for sample in short_dataset[\"ner_tags\"]:\n",
    "    label_list.append([label_encoding[tag] for tag in sample])\n",
    "\n",
    "\n",
    "print(label_list)\n",
    "# print(short_dataset[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_list = []\n",
    "\n",
    "for sample in val_dataset[\"ner_tags\"]:\n",
    "    val_label_list.append([label_encoding[tag] for tag in sample])\n",
    "\n",
    "test_label_list = []\n",
    "\n",
    "for sample in test_dataset[\"ner_tags\"]:\n",
    "    test_label_list.append([label_encoding[tag] for tag in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(short_dataset, list_name):\n",
    "    tokenized_inputs = tokenizer(short_dataset[\"tokens\"], truncation=True, is_split_into_words=True) ## For some models, you may need to set max_length to approximately 500.\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(list_name):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2005, 2023, 3800, 1996, 22836, 2402, 5381, 23011, 4094, 1006, 1043, 18863, 2015, 1007, 2001, 2764, 1012, 102], [101, 1996, 2206, 19389, 12955, 2020, 7594, 1024, 2358, 9626, 9080, 6204, 6651, 1006, 28177, 1010, 9587, 2140, 1044, 2475, 2080, 1049, 1011, 1016, 1055, 1011, 1015, 1007, 1010, 9099, 16781, 3446, 1006, 1041, 1010, 3461, 4747, 1044, 2475, 2080, 1049, 1011, 1016, 1055, 1011, 1015, 1007, 1010, 5658, 7760, 6038, 10760, 4588, 3446, 1006, 1052, 2078, 1010, 1166, 5302, 2140, 1049, 1011, 1016, 1055, 1011, 1015, 1007, 1998, 6970, 16882, 2522, 2475, 6693, 2522, 2475, 1006, 25022, 1010, 1166, 5302, 2140, 1049, 1011, 1016, 1055, 1011, 1015, 1007, 1012, 102], [101, 3576, 1044, 28873, 2035, 10448, 7382, 9816, 10960, 12192, 5258, 1999, 1996, 4292, 1997, 2529, 3393, 6968, 10085, 17250, 28873, 1006, 1044, 2721, 1007, 1516, 10349, 2035, 23924, 7416, 2278, 5024, 5812, 1998, 7872, 3526, 22291, 3370, 1006, 8040, 2102, 1007, 1031, 1017, 1010, 1018, 1033, 1012, 102], [101, 4958, 2072, 1027, 9052, 2933, 2906, 12126, 1012, 102], [101, 7297, 1010, 4372, 2891, 1011, 5173, 2053, 1055, 1011, 9152, 13181, 6508, 13776, 1156, 1011, 2552, 2378, 2006, 22330, 2015, 24434, 2549, 1998, 18234, 2552, 2378, 8031, 2000, 11268, 18622, 2078, 1011, 1015, 1006, 1052, 2546, 2078, 2487, 1007, 1010, 2004, 4484, 2007, 1996, 9099, 3490, 13181, 6508, 22248, 4005, 1055, 1011, 9152, 13181, 6499, 1011, 1048, 1011, 22330, 8602, 2063, 1006, 22330, 2015, 1011, 2053, 1007, 1012, 102], [101, 1996, 5197, 1997, 2053, 1998, 1996, 4195, 1997, 1052, 2546, 2078, 2487, 1011, 2552, 2378, 15420, 2006, 1996, 7816, 1997, 1052, 2243, 2278, 2001, 2522, 18933, 12821, 4383, 2011, 2058, 10288, 20110, 3258, 1997, 1011, 1162, 14376, 2078, 2487, 1011, 1998, 2552, 2378, 1011, 8031, 28829, 23892, 1997, 1156, 1011, 2552, 2378, 1006, 1039, 24434, 2549, 2015, 1007, 1998, 1052, 2546, 2078, 2487, 1006, 1044, 14526, 2683, 2063, 1007, 1010, 4414, 1010, 2029, 4359, 1996, 5317, 28964, 1997, 1052, 2243, 2278, 2012, 1996, 1011, 1162, 2278, 1011, 15488, 6305, 1012, 102], [101, 2122, 9556, 4895, 3726, 4014, 1037, 3117, 2053, 1011, 7790, 7337, 2011, 2029, 1996, 2552, 2378, 22330, 13122, 11705, 18903, 2078, 7711, 1996, 3029, 1998, 13791, 1997, 14828, 12702, 20464, 19966, 2545, 2012, 1996, 2003, 1012, 102], [101, 2119, 2033, 19658, 2509, 23616, 2080, 1998, 2037, 3748, 1011, 2828, 1006, 1059, 2102, 1007, 19070, 15416, 2020, 12599, 2012, 1041, 16048, 1012, 1019, 2007, 1019, 1011, 22953, 5302, 1011, 1016, 1521, 1011, 2139, 11636, 10513, 14615, 3170, 1006, 7987, 8566, 1007, 2000, 2650, 4442, 14996, 6064, 10752, 1012, 102], [101, 3522, 2147, 2011, 2149, 1998, 2500, 6083, 2008, 1996, 3677, 1521, 1055, 3684, 5213, 5250, 3938, 1006, 26236, 2361, 21057, 1007, 15775, 4842, 5643, 2064, 16913, 9869, 1996, 12761, 10425, 27797, 2011, 18191, 1031, 2324, 1010, 2539, 1033, 1012, 102], [101, 2057, 2864, 1037, 7399, 26237, 2944, 1997, 1996, 3466, 1997, 7473, 2140, 2006, 4487, 29212, 15822, 12612, 1006, 28144, 2072, 1007, 1012, 102], [101, 28144, 2072, 2001, 6022, 23900, 2000, 7473, 2140, 7644, 1010, 9104, 2008, 8244, 2007, 2062, 5729, 19637, 2094, 8030, 2036, 2018, 2062, 4487, 29212, 15822, 2004, 7203, 1999, 20965, 1016, 1006, 1054, 2475, 1027, 1014, 1012, 2654, 1010, 1052, 1026, 1014, 1012, 2199, 2487, 1007, 1012, 102], [101, 16902, 1997, 1006, 1037, 1007, 3446, 1997, 6534, 3742, 6622, 1998, 1006, 1038, 1007, 6534, 3742, 2007, 28699, 18963, 3446, 1006, 1165, 1007, 4358, 2011, 2478, 1054, 1024, 23192, 16478, 1997, 18612, 9031, 1006, 1054, 9739, 2850, 1007, 1999, 1996, 21697, 1011, 2241, 4106, 1012, 102], [101, 4050, 1010, 2012, 2659, 16021, 8197, 16259, 2100, 3893, 2250, 4576, 3778, 1006, 24531, 2361, 1007, 5300, 1010, 2460, 20228, 2497, 2094, 1998, 2659, 20228, 2497, 2361, 2024, 2275, 1010, 2012, 3020, 24531, 2361, 5300, 1010, 2936, 20228, 2497, 2094, 1998, 3020, 20228, 2497, 2361, 2024, 2275, 1012, 102], [101, 2043, 6195, 1996, 6413, 2791, 1997, 10449, 2019, 9353, 2361, 8778, 2012, 21117, 12458, 1010, 2009, 2001, 10358, 2008, 2045, 2001, 1037, 3906, 15074, 2006, 1996, 3451, 5328, 1997, 3737, 1997, 2331, 1006, 1053, 7716, 1007, 1999, 11959, 2021, 2048, 3025, 2913, 2013, 2642, 11959, 2018, 5393, 2008, 1053, 7716, 2001, 2019, 4493, 4145, 1999, 2642, 11959, 1012, 102], [101, 3893, 4442, 2013, 2169, 11375, 2020, 8897, 2013, 1037, 6263, 1997, 2176, 18154, 3479, 4249, 2566, 2152, 2373, 2492, 1006, 6522, 2546, 1007, 1998, 4102, 2007, 1996, 2491, 2011, 20177, 1996, 2779, 2193, 1997, 4442, 1012, 102], [101, 1042, 11020, 1010, 26899, 5806, 16902, 1025, 2720, 2078, 2615, 1010, 1049, 1012, 21069, 6137, 7293, 18891, 7946, 1025, 7605, 1010, 2093, 1011, 8789, 1012, 102], [101, 1999, 8469, 3372, 5510, 4442, 1010, 3670, 1997, 22597, 1998, 1996, 22597, 10769, 24312, 1155, 1006, 20868, 14608, 1007, 2038, 2042, 3818, 1031, 1015, 1033, 1010, 1031, 1016, 1033, 1012, 102], [101, 2000, 3231, 2023, 1010, 2057, 6086, 1996, 8000, 20014, 4355, 3170, 2000, 2048, 3785, 1024, 1015, 1007, 2382, 8117, 1997, 2003, 5403, 10092, 2628, 2011, 2382, 8117, 1997, 16360, 2121, 20523, 6911, 1010, 1998, 1016, 1007, 2003, 5403, 10092, 1013, 16360, 2121, 20523, 1006, 1045, 1013, 1054, 1007, 6911, 11211, 2007, 11320, 22311, 2140, 1999, 10085, 9513, 2007, 1052, 1012, 29347, 26549, 5740, 3736, 1006, 1045, 1009, 6643, 1007, 1012, 102], [101, 1996, 2334, 3989, 1997, 9587, 18724, 15420, 1010, 1998, 8558, 1996, 3257, 1997, 2929, 1010, 2003, 4340, 2011, 2019, 9808, 6895, 4571, 7062, 2291, 1010, 2029, 2950, 8203, 20292, 9587, 18724, 1037, 1006, 11460, 2721, 1007, 1010, 1037, 2235, 20710, 1011, 2066, 14181, 19707, 2063, 1010, 1998, 8203, 20292, 9587, 18724, 1038, 1006, 11460, 20850, 1007, 1010, 2049, 14181, 19707, 2063, 2552, 17441, 5250, 1006, 6578, 1007, 2004, 2350, 6177, 1031, 1015, 1010, 1018, 1010, 1019, 1033, 1012, 102], [101, 2478, 5072, 2892, 1011, 11383, 1013, 5796, 1010, 2057, 2018, 2042, 2583, 2000, 8080, 23758, 3370, 2389, 3431, 1999, 1996, 27854, 1011, 8031, 5884, 1006, 6053, 2094, 1007, 1997, 4903, 2906, 1011, 1155, 2588, 27854, 8031, 1012, 1031, 1022, 1033, 1999, 13578, 2094, 2240, 1048, 2620, 2001, 2055, 1996, 2087, 6802, 2240, 2000, 1996, 2093, 3231, 2545, 2241, 2006, 2119, 1043, 5104, 1012, 102], [101, 2004, 3491, 1999, 20965, 1015, 1010, 1996, 7403, 3292, 10035, 2090, 1996, 2358, 3089, 3654, 13070, 3210, 1998, 1996, 13070, 3231, 2121, 2020, 12368, 1999, 1996, 2846, 1997, 1043, 2094, 5300, 2084, 2216, 2090, 1996, 2358, 3089, 3654, 1011, 13070, 1998, 18002, 3231, 2121, 1010, 8131, 1996, 13070, 3231, 2121, 2001, 2641, 2004, 1037, 7218, 3231, 2121, 1012, 102], [101, 15050, 21733, 10572, 2011, 1038, 1012, 20934, 10623, 11592, 11124, 2003, 7790, 2006, 1996, 8290, 1997, 1996, 11867, 9711, 20318, 2063, 2007, 26632, 21890, 8449, 1998, 2060, 3526, 4127, 1010, 2107, 2004, 23915, 25930, 2102, 1006, 10710, 2102, 1007, 2030, 1056, 4442, 1031, 3486, 1033, 1012, 102], [101, 2951, 2024, 5228, 2004, 2812, 1081, 7367, 2213, 1010, 1050, 1027, 1017, 2013, 2012, 2560, 2093, 2981, 7885, 1025, 1050, 2094, 1024, 2025, 11156, 2030, 2917, 1996, 5787, 1997, 24110, 3775, 12516, 1012, 102], [101, 1038, 4328, 1024, 2303, 3742, 5950, 1025, 8292, 1024, 16480, 4244, 20902, 2140, 28517, 2099, 1025, 17163, 1024, 4958, 8939, 24587, 14834, 8331, 1025, 10768, 2615, 2487, 1024, 3140, 4654, 8197, 16259, 2100, 3872, 1999, 1015, 2117, 1025, 1042, 25465, 1024, 3140, 8995, 3977, 1025, 6335, 1024, 6970, 2571, 14228, 2078, 1025, 6845, 2050, 1024, 2146, 3772, 8247, 3283, 26942, 2015, 1025, 18832, 1024, 2146, 3772, 14163, 15782, 22612, 2278, 17379, 2015, 1025, 11338, 2361, 1011, 1015, 1024, 26632, 21890, 3351, 18178, 18938, 28804, 5250, 1011, 1015, 1025, 1056, 2290, 1024, 13012, 25643, 17119, 5178, 1012, 102], [101, 13366, 2546, 1024, 2640, 3466, 1025, 16461, 1024, 26721, 20464, 19966, 2121, 16902, 19064, 1025, 17371, 1024, 3115, 24353, 1025, 26264, 2099, 1024, 6970, 16211, 28228, 2571, 2846, 1012, 102], [101, 1996, 2877, 5320, 1997, 2331, 1998, 22822, 17062, 3012, 4969, 2024, 2512, 9006, 23041, 5555, 3468, 7870, 1006, 13316, 5104, 1007, 1010, 2107, 2004, 11888, 16464, 4295, 1010, 4456, 1010, 14671, 1010, 1998, 1010, 1999, 3327, 1010, 2003, 5403, 7712, 2540, 4295, 1998, 6909, 1031, 1015, 1033, 1012, 102], [101, 1996, 9663, 1997, 2492, 4654, 26243, 14049, 8466, 18279, 20746, 4022, 2015, 1006, 10768, 4523, 4523, 1007, 1006, 20965, 1020, 2497, 1007, 2001, 7594, 2000, 8080, 19962, 9331, 4588, 13791, 1997, 1996, 6187, 2487, 11918, 2389, 15698, 1012, 102], [101, 8670, 2480, 1010, 1038, 4328, 1011, 1038, 4328, 2005, 1011, 2287, 1062, 1011, 3556, 1025, 1056, 1010, 13012, 8625, 1011, 2000, 1011, 8915, 6494, 8625, 2122, 3565, 9006, 19386, 2229, 2024, 8924, 1997, 23060, 8524, 3366, 2007, 2593, 16464, 3375, 28954, 1998, 22330, 3406, 20366, 1039, 24087, 2475, 1006, 8040, 1024, 3523, 2549, 12848, 2475, 1010, 24829, 1024, 3523, 2549, 12848, 2549, 1007, 2030, 1997, 3375, 1010, 3523, 9006, 19386, 2072, 1998, 22330, 2102, 1006, 7842, 1024, 1045, 2487, 28954, 2549, 12848, 2549, 1025, 1055, 2487, 1010, 1055, 2509, 1998, 1055, 2549, 20965, 2015, 1007, 1012, 102], [101, 22498, 2015, 1024, 14931, 1010, 5402, 11207, 1025, 1050, 1013, 1037, 1010, 2025, 12711, 1025, 1050, 2487, 1010, 18906, 2015, 1011, 18906, 2015, 2522, 2615, 1011, 1016, 16371, 14321, 24755, 4523, 3593, 4962, 2555, 1015, 1025, 1011, 2522, 2615, 1011, 1016, 10286, 2015, 1010, 5729, 11325, 16464, 8715, 21887, 23350, 1016, 1025, 4895, 2243, 1010, 4242, 2000, 2582, 6709, 3278, 16902, 2015, 2090, 6887, 16515, 13874, 2015, 1998, 2175, 8474, 1010, 2057, 2109, 1037, 2175, 2744, 2424, 2121, 4007, 1031, 4749, 1033, 2000, 2522, 14343, 13806, 6887, 16515, 13874, 2015, 2007, 2175, 2478, 1996, 1052, 7011, 2213, 2000, 2175, 12375, 2951, 2013, 1996, 4962, 3031, 6483, 12360, 1012, 102], [101, 13587, 5423, 3593, 14817, 1006, 18856, 2063, 1999, 11460, 1007, 2001, 4358, 2206, 9226, 22513, 3802, 2632, 1012, 102], [101, 1006, 10476, 1007, 1998, 5423, 3593, 4180, 1006, 29215, 1007, 2001, 4358, 1999, 11460, 1012, 102], [101, 1048, 1011, 1015, 1012, 102], [101, 2028, 2154, 2044, 1996, 7547, 1010, 14704, 8578, 2020, 9099, 28901, 2007, 16298, 2080, 1011, 3378, 7865, 1006, 9779, 2615, 1007, 14262, 26305, 1023, 9309, 17181, 11503, 3211, 2072, 1012, 1045, 23296, 2226, 2226, 1012, 102], [101, 1059, 28139, 1011, 1044, 5603, 1031, 2382, 1033, 1012, 102], [101, 14557, 20647, 13181, 3366, 12943, 2906, 1006, 22851, 2050, 1007, 1998, 14557, 20647, 13181, 3366, 22953, 2705, 1006, 22851, 2497, 1007, 2020, 4156, 2013, 4487, 11329, 2080, 1006, 1038, 2094, 1010, 3915, 1007, 1012, 102], [101, 13853, 26427, 1006, 6187, 3597, 2509, 1007, 1998, 22886, 2020, 4156, 2013, 25353, 6238, 2213, 1006, 28904, 1010, 6027, 1007, 1012, 102], [101, 4358, 1043, 21297, 2121, 7934, 10882, 7096, 8156, 6165, 1006, 1041, 25708, 2869, 1007, 2020, 10174, 2478, 1996, 2176, 1011, 8023, 8522, 5173, 2013, 1996, 14080, 1997, 8738, 1999, 25125, 4295, 1006, 9108, 4103, 1007, 2817, 1012, 102], [101, 2057, 2109, 1996, 2892, 1011, 27197, 2951, 2164, 1020, 1010, 8148, 4587, 2214, 1006, 4793, 3770, 2086, 2214, 2030, 3080, 1007, 2013, 1996, 7403, 4942, 3367, 20217, 1997, 1996, 2822, 20134, 7965, 26906, 5002, 1006, 18856, 7317, 2015, 1007, 2029, 2003, 1037, 2120, 2898, 2522, 27794, 2817, 2008, 2211, 1999, 2687, 2007, 3582, 1011, 2039, 12265, 2296, 1016, 1516, 1017, 2086, 1012, 102], [101, 10889, 1010, 2348, 6335, 1011, 2570, 20544, 2015, 4742, 9099, 8566, 17119, 1998, 2552, 11444, 4263, 1997, 14193, 1017, 1006, 28093, 2509, 1007, 1999, 9706, 2278, 1011, 15527, 4442, 1010, 28093, 2509, 4539, 9165, 2020, 2025, 10572, 1012, 102], [101, 1044, 2232, 1010, 17834, 25852, 1025, 17212, 2546, 2475, 1010, 4517, 5387, 1011, 9413, 22123, 8093, 9314, 1016, 1011, 2066, 1016, 10047, 19797, 2509, 1010, 5110, 19960, 22187, 2854, 9334, 23245, 1025, 1050, 2497, 1010, 28991, 23684, 1025, 15488, 2080, 1010, 5744, 6675, 1012, 102], [101, 4919, 1010, 1996, 2512, 1011, 16731, 2278, 6818, 2443, 13138, 2007, 3806, 13181, 18447, 19126, 10722, 20360, 1006, 21025, 2102, 1007, 1010, 6421, 2007, 28378, 11290, 10722, 20360, 1006, 1038, 7096, 1007, 1010, 3429, 2509, 2007, 11290, 25022, 12171, 25229, 1010, 2676, 2007, 11888, 28389, 1039, 1006, 10381, 2278, 1007, 1010, 6564, 2007, 11888, 28389, 1038, 1006, 10381, 2497, 1007, 1010, 2753, 2007, 28378, 3806, 13181, 18447, 19126, 4295, 1006, 1038, 5856, 2094, 1007, 1998, 21679, 7965, 2111, 1012, 102], [101, 2000, 9699, 19701, 2035, 26741, 1010, 2057, 4912, 2000, 3443, 23892, 2007, 2312, 15778, 3972, 20624, 5644, 2011, 12697, 1016, 2309, 5009, 12987, 2015, 1006, 22214, 12789, 2015, 1007, 2306, 2030, 24958, 1996, 7872, 1011, 7077, 2555, 1006, 20965, 26314, 1007, 1012, 102], [101, 2478, 7473, 2099, 8991, 4140, 22571, 2075, 1010, 2057, 5147, 4453, 4264, 4755, 14494, 1999, 2296, 14719, 16576, 2475, 2155, 2266, 1999, 1996, 1056, 2487, 4245, 1006, 20965, 26314, 1025, 1055, 2487, 20965, 1007, 1012, 102], [101, 14246, 1010, 1041, 5092, 2615, 11255, 1041, 5092, 2615, 1043, 2135, 3597, 21572, 9589, 1012, 102], [101, 2057, 2109, 1037, 3563, 27781, 2114, 1996, 20877, 6672, 4135, 5666, 4588, 25468, 1006, 7610, 2140, 1007, 5250, 2000, 2817, 1996, 6580, 1997, 1996, 7610, 2140, 4517, 4230, 1010, 2029, 2024, 2092, 1011, 7356, 4942, 11231, 14321, 2906, 5090, 1031, 2570, 1033, 1012, 102], [101, 1999, 5903, 1010, 1996, 3446, 1997, 11604, 11219, 2930, 1006, 1039, 1011, 2930, 1007, 23534, 2038, 2036, 3445, 12099, 2058, 1996, 2197, 5109, 1012, 102], [101, 1528, 1054, 4877, 11616, 2001, 2081, 2006, 1996, 16474, 9181, 1997, 1996, 2248, 15035, 3456, 8715, 2817, 2177, 1006, 2230, 1007, 1012, 102], [101, 26264, 2099, 1024, 6970, 16211, 28228, 2571, 2846, 6660, 1010, 7667, 1997, 2060, 20155, 10857, 2003, 6827, 1999, 3653, 1011, 1998, 2695, 1011, 2771, 3785, 2029, 3952, 2950, 1010, 12123, 18423, 3798, 1010, 2303, 3742, 5950, 1006, 1038, 4328, 1007, 1010, 2152, 4304, 5423, 3593, 1006, 10751, 2140, 1007, 1010, 2659, 4304, 5423, 3593, 1006, 25510, 2140, 1007, 1998, 13012, 25643, 17119, 8621, 3798, 1012, 102], [101, 4840, 9980, 6731, 1999, 1014, 1012, 1015, 1003, 17712, 15185, 2020, 9358, 3089, 11263, 5999, 2005, 2321, 8117, 2012, 5385, 11575, 1998, 3202, 6731, 1999, 1016, 19968, 1997, 4241, 20850, 8586, 3597, 1005, 1055, 6310, 6755, 1005, 1055, 1006, 1040, 4168, 2213, 1007, 1009, 18667, 2050, 2692, 1012, 1015, 1003, 1012, 102], [101, 1999, 2804, 1010, 8915, 2213, 3896, 2006, 29215, 2509, 1011, 2462, 3798, 2020, 2036, 6560, 1999, 1019, 1005, 16298, 20049, 2638, 18847, 8458, 2891, 24556, 1011, 8878, 5250, 21903, 1006, 23713, 2243, 1007, 1155, 2487, 1013, 1155, 2475, 3313, 12849, 2033, 10343, 1006, 1055, 2575, 2497, 20965, 1007, 1012, 102], [101, 11888, 1044, 2497, 2615, 8985, 2001, 6219, 2429, 2000, 1996, 6745, 2647, 2523, 2005, 1996, 2817, 1997, 1996, 11290, 1006, 19413, 14540, 1007, 6612, 3218, 11594, 1006, 18133, 2290, 1007, 1031, 1015, 1033, 1012, 102], [101, 22498, 2015, 1024, 25022, 1010, 7023, 13483, 1025, 26419, 2072, 1010, 19686, 4167, 4544, 1012, 102], [101, 2023, 2817, 2003, 2988, 2004, 2566, 1996, 10495, 4781, 1997, 7316, 7012, 1006, 13440, 1007, 1006, 1055, 2487, 13440, 4638, 9863, 1007, 1012, 102], [101, 1996, 3891, 1997, 8909, 7457, 2076, 10032, 2349, 2000, 2019, 3623, 1999, 11062, 3707, 5918, 2000, 8752, 1996, 4935, 1997, 11062, 2417, 2668, 3526, 1006, 21144, 2278, 1007, 3742, 1010, 2458, 1997, 1996, 2173, 12380, 1998, 10768, 5809, 1010, 1998, 1996, 3279, 1997, 2668, 3378, 2007, 4450, 1998, 6959, 1031, 1016, 1033, 1012, 102], [101, 2023, 2817, 2003, 2988, 2429, 2000, 1996, 16003, 1996, 7316, 1997, 8089, 2389, 2913, 1999, 4958, 5178, 4328, 6779, 1006, 2358, 3217, 4783, 1007, 4861, 1006, 1055, 2487, 2358, 3217, 4783, 4638, 9863, 1007, 1012, 102], [101, 8040, 28433, 4588, 3778, 7774, 1997, 1037, 12170, 4502, 2361, 19536, 1006, 1037, 1007, 4102, 2000, 19536, 1999, 20228, 2497, 2615, 5549, 1006, 1038, 1007, 1012, 102], [101, 12170, 4502, 2361, 1024, 23974, 15985, 3893, 2250, 4576, 3778, 1025, 24531, 2361, 1024, 16021, 8197, 16259, 2100, 3893, 2250, 4576, 3778, 1025, 19044, 2361, 1024, 4654, 8197, 16259, 2100, 3893, 2250, 4576, 3778, 1025, 20228, 2497, 2615, 1024, 21954, 2970, 5505, 19536, 1012, 102], [101, 13365, 13853, 3863, 2869, 1006, 13316, 2595, 1007, 2024, 2659, 16730, 2152, 3977, 1006, 21117, 4017, 2456, 1516, 13509, 1055, 27944, 1007, 3424, 6442, 2545, 2008, 2064, 5901, 2693, 6187, 2475, 1009, 15956, 2408, 1996, 12123, 10804, 1012, 102], [101, 4442, 2020, 2187, 2000, 7818, 2005, 1017, 1516, 1018, 5486, 2420, 2000, 2433, 1037, 9530, 10258, 24997, 2102, 18847, 24314, 1006, 4983, 3090, 4728, 1007, 2077, 8081, 3370, 2007, 1018, 1003, 11498, 14192, 19058, 11106, 18124, 1006, 1052, 7011, 1007, 2001, 2864, 1012, 102], [101, 9308, 1010, 7632, 2546, 1011, 1016, 14608, 1516, 7790, 2039, 1011, 7816, 1997, 6970, 2571, 14228, 2078, 1006, 6335, 1007, 1011, 1020, 1999, 13109, 2015, 25194, 20582, 1997, 16215, 16576, 4442, 1517, 10232, 3466, 5668, 1997, 10958, 26835, 19009, 1012, 102], [101, 2057, 7718, 1016, 2659, 1011, 8015, 4617, 1997, 17324, 2102, 1999, 1037, 3979, 1006, 8546, 2969, 1011, 2393, 1998, 3569, 3274, 5084, 10699, 1011, 9164, 7242, 1031, 10507, 19279, 1033, 1007, 1010, 5604, 2037, 4254, 2006, 5022, 2007, 1051, 19797, 2043, 3024, 3188, 2000, 17324, 2102, 2007, 1037, 19294, 1012, 102], [101, 1996, 21877, 2140, 7903, 15824, 15099, 1011, 15219, 6705, 1006, 9706, 2094, 1007, 1010, 2036, 2170, 1996, 27885, 13473, 12412, 9530, 9103, 5867, 1010, 2001, 7594, 2013, 1996, 15099, 2522, 28228, 9289, 3302, 1997, 1996, 17266, 17597, 19731, 10024, 2140, 4850, 1006, 19430, 13663, 2854, 1007, 2000, 1996, 7541, 2391, 2006, 1996, 18309, 15219, 1011, 6020, 7814, 1997, 1996, 9047, 2483, 25353, 8737, 10536, 6190, 1012, 102], [101, 2011, 5688, 1010, 9412, 2665, 1011, 10077, 5250, 1011, 20446, 8978, 11483, 1006, 1041, 25708, 2361, 1011, 12395, 1007, 2003, 1037, 2172, 16176, 1010, 28991, 5302, 8017, 16730, 3283, 26942, 1997, 1996, 10442, 1038, 2475, 2099, 2008, 2515, 2025, 14187, 2000, 9078, 1031, 1023, 1033, 1012, 102], [101, 2747, 2045, 2003, 2053, 3115, 2117, 1011, 2240, 3949, 2005, 26261, 22943, 1011, 25416, 22648, 7062, 12943, 2615, 14945, 1006, 5034, 1011, 12943, 2615, 14945, 1007, 1012, 102], [101, 3365, 9942, 2000, 7438, 5034, 1011, 12943, 2615, 14945, 2031, 2042, 2988, 1010, 2021, 3463, 2031, 2042, 15640, 2007, 9671, 3433, 6165, 1998, 2146, 2744, 3452, 7691, 1006, 9808, 1007, 1997, 2069, 2322, 1516, 2382, 1003, 1031, 1016, 1033, 1012, 102], [101, 6315, 2951, 2005, 9320, 1037, 1998, 1038, 2064, 2022, 2179, 1999, 1996, 4637, 2592, 2930, 1055, 2475, 2951, 5371, 1012, 102], [101, 4718, 14277, 1010, 1050, 1011, 2433, 8516, 11368, 4048, 16585, 2140, 1011, 3393, 14194, 8516, 1011, 6887, 2368, 23943, 21141, 2638, 1025, 1047, 2094, 1010, 7324, 7698, 1025, 24978, 4095, 12789, 1010, 2512, 13102, 8586, 18513, 14021, 12789, 14021, 12789, 1012, 102], [101, 2000, 5646, 3251, 2023, 20187, 3433, 2003, 7686, 2011, 2039, 1011, 7816, 1997, 4013, 1011, 20187, 16387, 1010, 2057, 2864, 7901, 14193, 7473, 2099, 1006, 19387, 1011, 7473, 2099, 1007, 2006, 9673, 4755, 1037, 8590, 2175, 3468, 2102, 3526, 10859, 1010, 1998, 5262, 2057, 2156, 3445, 3798, 1997, 28286, 2546, 14608, 1010, 1039, 2595, 20464, 2487, 1010, 6335, 2620, 1010, 2065, 2078, 2487, 1010, 1998, 6335, 2487, 29720, 1999, 2122, 9673, 1006, 3275, 1055, 2487, 2050, 1007, 1012, 102], [101, 9308, 1010, 2065, 2057, 2298, 2011, 20155, 7473, 2099, 1006, 1053, 15042, 2099, 1007, 2012, 3798, 1997, 2048, 1997, 2122, 9165, 1010, 6335, 2487, 29720, 1998, 1039, 2595, 20464, 2487, 1010, 2057, 2156, 2037, 3154, 15946, 1999, 9673, 2073, 1058, 12521, 8180, 3670, 2003, 10572, 2011, 3684, 5213, 2069, 1020, 1044, 2077, 12987, 14676, 1010, 9104, 2008, 4013, 1011, 20187, 4962, 3670, 2003, 1037, 5915, 3433, 2000, 1058, 12521, 8180, 3670, 1006, 3275, 1055, 2487, 2497, 1007, 1012, 102], [101, 6419, 1050, 16118, 7315, 2011, 2079, 7630, 2618, 17643, 21663, 1006, 26718, 2290, 1007, 2089, 13730, 1996, 3659, 1997, 5012, 1010, 2021, 2009, 2003, 10599, 2129, 4621, 2023, 6942, 2097, 2022, 1998, 2029, 5776, 2967, 2323, 2022, 7237, 2013, 1050, 16118, 3775, 2000, 26718, 2290, 1012, 102], [101, 24051, 8462, 24558, 4106, 11156, 10114, 11658, 2135, 5228, 9165, 1006, 2139, 5620, 1007, 3141, 2000, 23060, 8524, 6024, 6911, 1012, 102], [101, 2633, 1010, 15982, 2592, 2001, 5067, 1010, 2164, 1996, 6377, 11778, 2729, 2005, 3080, 2111, 1006, 2003, 16186, 1007, 3160, 20589, 1010, 2000, 5646, 3375, 2740, 3471, 1999, 1996, 6818, 1031, 3486, 1033, 1012, 102], [101, 1996, 7975, 3544, 2000, 3768, 1996, 2030, 2705, 12898, 2290, 2005, 1996, 2529, 11721, 3676, 19892, 1006, 13091, 13096, 8569, 3723, 7277, 5648, 1516, 8031, 10769, 1007, 1015, 1012, 102], [101, 12328, 1022, 1516, 2184, 3134, 2214, 10372, 26721, 4842, 9956, 22084, 9215, 2007, 9402, 1006, 1037, 1004, 1041, 1007, 1010, 9800, 1006, 1038, 1004, 1042, 1007, 1010, 9645, 1006, 1039, 1004, 1043, 1007, 1010, 2030, 10114, 1006, 1040, 1004, 1044, 1007, 11952, 5716, 3197, 1006, 1052, 11263, 1007, 3044, 10178, 11338, 2213, 2615, 2018, 15965, 2668, 18847, 11231, 14321, 2906, 4442, 1006, 1052, 25526, 2278, 1007, 16330, 2385, 3134, 2044, 8985, 2005, 14200, 5649, 11338, 2213, 2615, 1011, 3563, 1056, 1011, 4442, 1012, 102], [101, 7561, 6963, 2020, 5020, 2000, 1996, 3115, 7561, 1997, 1996, 2812, 1006, 7367, 2213, 1007, 1997, 1037, 16745, 1997, 1996, 2334, 21318, 2015, 1012, 102], [101, 1996, 16745, 2001, 4217, 2061, 2008, 2053, 2048, 27189, 4207, 2019, 3341, 1010, 12725, 2008, 2334, 16902, 2015, 1999, 1996, 21318, 2106, 2025, 7976, 2135, 5547, 1996, 7367, 2213, 1012, 102], [101, 1996, 7367, 2213, 2001, 9398, 4383, 2478, 1037, 6879, 6494, 2361, 12504, 1012, 102], [101, 2057, 2424, 2008, 23996, 14996, 8394, 2331, 8176, 20275, 3653, 4890, 13490, 1997, 8171, 3223, 2005, 5292, 6633, 8649, 4135, 8428, 2039, 15166, 1998, 1996, 16012, 23737, 1997, 1996, 17886, 3512, 12436, 10841, 9890, 1006, 1040, 2615, 1007, 1012, 102], [101, 2057, 2582, 10580, 28829, 2334, 6648, 1997, 3653, 4890, 13776, 11626, 2865, 6591, 1010, 28829, 2039, 15166, 1997, 5292, 6633, 8649, 4135, 8428, 1010, 1998, 1037, 20275, 1040, 2615, 1999, 2122, 23996, 1012, 102], [101, 15877, 1006, 26419, 1007, 2003, 2028, 1997, 1996, 2877, 5320, 1997, 2331, 2013, 16514, 4295, 4969, 1010, 1998, 2009, 8563, 2105, 1015, 1012, 1021, 2454, 2111, 2169, 2095, 1012, 102], [101, 1999, 4766, 1010, 1037, 11038, 2433, 1997, 19802, 6190, 1011, 3141, 5812, 4945, 7667, 1006, 10682, 1007, 3556, 1031, 2603, 1033, 1011, 2241, 4577, 2291, 1010, 1996, 18856, 10128, 1011, 1039, 1997, 3556, 1010, 2001, 2109, 2000, 16157, 1997, 1012, 102], [101, 1996, 18856, 10128, 1011, 6187, 20464, 2546, 3556, 2001, 2764, 2011, 11566, 1996, 18856, 10128, 1011, 2522, 2546, 3556, 1998, 2048, 2060, 2981, 4013, 26745, 10074, 5876, 1010, 2287, 1010, 1998, 2317, 2668, 3526, 1006, 25610, 2278, 1007, 4175, 1031, 2570, 1033, 1012, 102], [101, 1996, 2093, 1011, 16381, 2944, 2008, 2106, 2025, 13265, 6970, 1011, 8875, 24004, 6914, 20175, 2100, 1006, 2004, 3491, 1999, 3275, 1022, 2050, 1007, 14729, 2005, 2069, 4868, 1003, 1997, 1996, 23284, 1006, 1054, 1016, 1007, 1010, 1998, 2018, 2019, 9875, 17339, 2592, 19229, 1006, 9932, 2278, 1007, 3643, 1006, 2156, 4725, 1007, 1997, 1015, 1010, 5890, 2692, 1006, 3020, 9932, 2278, 5300, 5769, 4788, 4906, 1031, 2676, 1033, 1007, 1012, 102], [101, 1040, 17134, 1010, 16510, 27710, 5402, 2410, 28086, 2015, 1025, 6574, 2475, 1010, 2130, 16791, 18247, 1016, 1025, 25188, 1010, 10424, 27225, 14117, 2102, 1025, 1044, 16932, 1010, 5292, 24759, 9314, 5402, 2403, 28086, 2015, 1025, 14161, 2072, 1010, 14161, 4313, 4523, 1025, 1050, 1010, 6463, 1997, 23767, 2000, 22330, 14399, 8523, 2213, 1025, 7473, 2532, 1010, 4013, 15509, 15172, 3526, 4517, 28873, 1012, 102], [101, 2057, 14925, 14399, 15004, 5228, 1050, 1012, 13675, 12054, 2050, 16405, 2546, 2509, 5250, 19660, 2000, 1037, 18231, 16730, 28406, 6415, 1006, 11112, 1011, 6415, 1007, 1999, 1037, 1055, 1012, 8292, 2890, 11365, 19001, 10178, 4394, 2203, 23924, 3560, 16405, 2546, 8171, 1010, 16405, 2546, 2487, 1011, 1019, 1006, 5173, 2013, 1019, 29722, 14289, 10343, 10178, 1031, 4700, 1033, 1007, 1010, 1998, 2057, 4453, 1996, 12987, 2015, 5391, 2011, 1050, 1012, 13675, 12054, 2050, 16405, 2546, 2509, 1006, 4475, 1998, 4725, 1007, 1012, 102], [101, 2023, 15861, 3012, 2001, 10358, 2043, 2057, 13115, 2035, 16861, 4655, 2000, 1996, 2034, 16371, 14321, 19137, 4168, 13248, 1997, 1996, 24529, 2015, 1998, 4396, 2035, 2122, 9165, 2011, 1037, 16371, 14321, 19137, 4168, 19120, 15861, 3012, 1006, 27937, 2361, 1007, 3556, 4162, 2000, 1996, 16861, 2555, 1006, 3275, 23842, 1025, 4475, 1998, 4725, 1007, 1012, 102], [101, 3568, 1010, 1996, 2817, 6461, 2000, 6592, 1996, 5072, 5512, 1997, 1039, 1012, 27885, 5809, 2050, 7053, 14817, 2478, 29215, 1011, 5796, 1012, 102], [101, 2000, 14358, 2065, 1010, 4661, 1996, 3424, 11636, 8524, 3372, 4023, 1010, 1039, 1012, 27885, 5809, 2050, 2052, 8327, 2019, 3424, 12520, 3466, 1998, 2029, 10595, 2052, 2022, 2920, 1999, 2009, 1010, 2057, 16330, 1996, 3466, 1997, 1996, 14817, 2006, 17710, 8609, 5740, 27321, 1998, 10882, 12618, 28522, 12837, 6086, 2000, 23068, 8249, 1998, 16578, 5250, 6351, 8516, 1010, 8902, 20679, 2078, 1010, 1044, 21095, 21017, 2594, 5648, 1006, 5292, 1007, 1010, 3384, 4135, 21572, 9589, 11022, 1011, 1015, 1006, 3461, 2361, 1011, 1015, 1007, 1010, 6335, 1011, 1015, 1156, 1998, 6335, 1011, 1020, 8417, 1012, 102], [101, 2057, 2034, 16578, 5966, 2090, 11682, 2006, 1996, 4023, 7236, 1024, 3622, 5776, 2729, 1006, 1040, 15042, 1007, 1010, 7268, 5776, 2729, 1006, 28569, 1007, 1010, 2236, 8518, 1006, 14181, 1007, 1998, 2060, 8518, 1006, 27178, 1007, 1012, 102], [101, 5512, 2389, 4106, 4487, 25572, 4570, 2008, 2028, 8023, 3791, 2000, 2022, 4217, 2004, 1037, 4431, 8023, 1010, 2000, 12826, 1996, 2500, 2114, 1012, 102], [101, 4919, 1010, 2057, 8209, 2128, 13770, 2015, 2478, 1996, 9078, 3406, 18037, 11368, 29598, 28517, 2099, 1006, 2572, 1011, 28517, 2099, 1007, 1997, 2023, 18554, 1006, 2156, 4725, 1007, 1010, 1998, 2059, 2057, 27846, 4487, 24137, 7405, 3064, 3265, 8473, 6302, 2890, 3401, 13876, 5668, 1998, 1013, 2030, 9808, 2015, 1012, 102], [101, 2057, 7021, 1996, 7547, 2478, 20868, 21203, 2012, 9683, 13221, 1998, 2019, 20868, 1011, 7591, 2678, 4950, 4987, 2000, 1996, 24635, 1012, 102], [101, 2005, 2070, 2086, 1010, 1996, 4555, 23370, 3446, 1006, 2720, 2099, 1007, 1997, 1996, 16021, 8197, 16259, 2100, 6650, 2038, 2042, 2109, 2004, 2019, 14958, 12115, 1997, 16342, 1998, 2058, 11066, 1997, 1996, 16021, 8197, 16259, 2100, 6650, 1012, 102], [101, 9327, 3802, 2632, 1012, 102], [101, 1031, 2385, 1033, 7645, 2008, 14021, 2361, 2475, 7711, 6887, 2891, 8458, 10253, 13490, 1998, 13791, 1997, 5034, 2278, 2155, 21903, 2015, 1006, 16420, 5705, 1007, 1010, 1999, 2008, 16420, 2243, 4023, 2003, 4359, 1999, 1996, 6438, 1997, 14021, 2361, 2475, 1010, 2349, 2000, 23760, 8458, 2891, 8458, 10253, 13490, 1997, 1996, 24054, 2100, 6887, 2891, 8458, 10253, 13490, 2609, 1012, 102], [101, 14089, 1010, 15053, 23058, 2099, 5516, 16226, 1025, 11867, 15671, 2683, 1010, 2358, 2890, 13876, 24163, 27631, 1052, 7677, 6914, 2229, 25222, 2683, 1025, 1059, 2102, 1010, 3748, 1011, 2828, 1012, 102], [101, 14405, 13492, 3490, 10415, 5648, 1006, 9779, 1007, 12448, 3370, 26001, 9535, 12604, 9307, 1998, 1040, 2546, 1012, 102], [101, 1006, 1037, 1010, 1038, 1007, 4297, 19761, 3508, 1997, 14595, 2080, 1011, 1016, 1006, 12987, 2072, 1007, 16253, 1999, 1019, 3461, 9779, 5260, 2000, 9535, 12604, 9307, 19857, 20030, 2714, 2000, 3748, 2828, 1012, 102], [101, 1006, 1037, 1007, 3466, 2006, 9535, 12604, 9307, 19857, 20030, 1997, 4297, 19761, 3508, 1999, 1037, 2846, 1997, 9779, 14061, 1012, 102], [101, 1019, 3461, 9779, 3957, 19857, 20030, 3798, 2714, 2000, 3748, 2828, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 3, 3, 3, 0, -100], [-100, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 0, 0, -100], [-100, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -100], [-100, 0, 0, 0, 0, 1, 1, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 3, 3, 0, 1, 1, 0, 0, -100], [-100, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 3, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 2, 2, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 2, 3, 3, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 3, 0, 1, 0, 2, 2, 2, 2, 3, 3, 0, 1, 0, 2, 3, 0, 1, 1, 0, 2, 2, 2, 2, 3, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 2, 3, 1, 1, 3, 3, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 0, -100], [-100, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 2, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 1, 1, 1, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 2, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 0, 2, 2, 2, 3, 1, 1, 1, 3, 3, 3, 3, 3, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 2, 2, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 1, 0, 2, 3, 0, 1, 1, 0, 2, 3, 3, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 1, 0, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 2, 3, 3, 3, 0, -100], [-100, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 3, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, -100], [-100, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 0, 1, 1, 1, 0, 2, 2, 2, 2, 3, 3, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 1, 1, 1, 0, 2, 2, 2, 2, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100], [-100, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, -100], [-100, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, -100], [-100, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 3, 3, 3, 0, 1, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 0, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 0, 1, 0, 0, 2, 3, 0, 1, 0, 0, 2, 3, 0, 1, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 0, 2, 2, 2, 3, 3, 0, 1, 1, 1, 0, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 3, 3, 0, -100], [-100, 2, 2, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, -100], [-100, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenize_and_align_labels(short_dataset, label_list)\n",
    "print(tokenized_datasets)\n",
    "# print(i[0] for i in tokenized_datasets[\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_val_datasets = tokenize_and_align_labels(val_dataset, val_label_list)\n",
    "tokenized_test_datasets = tokenize_and_align_labels(test_dataset, test_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 4\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (4.66.2)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (2.2.1)\n",
      "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
      "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "   ---------------------------------------- 0.0/290.1 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 71.7/290.1 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 290.1/290.1 kB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "args = TrainingArguments(\n",
    "    f\"BERT-finetuned-NER\",\n",
    "    # evaluation_strategy = \"epoch\", ## Instead of focusing on loss and accuracy, we will focus on the F1 score\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 7000,\n",
    "    save_total_limit = 3,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.001,\n",
    "    save_steps=35000,\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/43.6 kB 640.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.6/43.6 kB 529.6 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from seqeval) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.3.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16185 sha256=842d7b88b1cf707a7bec1c8e29c16d7903d9d62d1d3dd4d18e3077ee51934570\n",
      "  Stored in directory: c:\\users\\gk00554\\appdata\\local\\pip\\cache\\wheels\\1a\\67\\4a\\ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_dict_to_list_of_dict(d):\n",
    "    new_list = []\n",
    "\n",
    "    for labels, inputs in zip(d[\"labels\"], d[\"input_ids\"]):\n",
    "        entry = {\"input_ids\": inputs, \"labels\": labels}\n",
    "        new_list.append(entry)\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-100, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 3, 3, 3, 0, -100], [-100, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 0, 0, -100], [-100, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -100], [-100, 0, 0, 0, 0, 1, 1, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 3, 3, 0, 1, 1, 0, 0, -100], [-100, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 3, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 2, 2, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 2, 3, 3, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 3, 0, 1, 0, 2, 2, 2, 2, 3, 3, 0, 1, 0, 2, 3, 0, 1, 1, 0, 2, 2, 2, 2, 3, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 2, 3, 1, 1, 3, 3, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 0, -100], [-100, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 2, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 1, 1, 1, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 2, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 0, 2, 2, 2, 3, 1, 1, 1, 3, 3, 3, 3, 3, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 2, 2, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 1, 0, 2, 3, 0, 1, 1, 0, 2, 3, 3, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 1, 0, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 2, 3, 3, 3, 0, -100], [-100, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 3, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, -100], [-100, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 0, 1, 1, 1, 0, 2, 2, 2, 2, 3, 3, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 1, 1, 1, 0, 2, 2, 2, 2, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100], [-100, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, -100], [-100, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, -100], [-100, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 3, 3, 3, 0, 1, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 0, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 0, 1, 0, 0, 2, 3, 0, 1, 0, 0, 2, 3, 0, 1, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 1, 0, 2, 2, 2, 3, 3, 0, 1, 1, 1, 0, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 3, 3, 0, -100], [-100, 2, 2, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, -100], [-100, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]\n"
     ]
    }
   ],
   "source": [
    "# print(tokenized_datasets[\"labels\"])\n",
    "\n",
    "# new = []\n",
    "\n",
    "# for labels, inputs in zip(tokenized_datasets[\"labels\"], tokenized_datasets[\"input_ids\"]):\n",
    "#     entry = {\"input_ids\": inputs, \"labels\": labels}\n",
    "#     new.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_train = turn_dict_to_list_of_dict(tokenized_datasets)\n",
    "tokenised_val = turn_dict_to_list_of_dict(tokenized_val_datasets)\n",
    "tokenised_test = turn_dict_to_list_of_dict(tokenized_test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [101, 2005, 2023, 3800, 1996, 22836, 2402, 5381, 23011, 4094, 1006, 1043, 18863, 2015, 1007, 2001, 2764, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1996, 2206, 19389, 12955, 2020, 7594, 1024, 2358, 9626, 9080, 6204, 6651, 1006, 28177, 1010, 9587, 2140, 1044, 2475, 2080, 1049, 1011, 1016, 1055, 1011, 1015, 1007, 1010, 9099, 16781, 3446, 1006, 1041, 1010, 3461, 4747, 1044, 2475, 2080, 1049, 1011, 1016, 1055, 1011, 1015, 1007, 1010, 5658, 7760, 6038, 10760, 4588, 3446, 1006, 1052, 2078, 1010, 1166, 5302, 2140, 1049, 1011, 1016, 1055, 1011, 1015, 1007, 1998, 6970, 16882, 2522, 2475, 6693, 2522, 2475, 1006, 25022, 1010, 1166, 5302, 2140, 1049, 1011, 1016, 1055, 1011, 1015, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 3576, 1044, 28873, 2035, 10448, 7382, 9816, 10960, 12192, 5258, 1999, 1996, 4292, 1997, 2529, 3393, 6968, 10085, 17250, 28873, 1006, 1044, 2721, 1007, 1516, 10349, 2035, 23924, 7416, 2278, 5024, 5812, 1998, 7872, 3526, 22291, 3370, 1006, 8040, 2102, 1007, 1031, 1017, 1010, 1018, 1033, 1012, 102], 'labels': [-100, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 4958, 2072, 1027, 9052, 2933, 2906, 12126, 1012, 102], 'labels': [-100, 1, 1, 0, 2, 3, 3, 3, 0, -100]}, {'input_ids': [101, 7297, 1010, 4372, 2891, 1011, 5173, 2053, 1055, 1011, 9152, 13181, 6508, 13776, 1156, 1011, 2552, 2378, 2006, 22330, 2015, 24434, 2549, 1998, 18234, 2552, 2378, 8031, 2000, 11268, 18622, 2078, 1011, 1015, 1006, 1052, 2546, 2078, 2487, 1007, 1010, 2004, 4484, 2007, 1996, 9099, 3490, 13181, 6508, 22248, 4005, 1055, 1011, 9152, 13181, 6499, 1011, 1048, 1011, 22330, 8602, 2063, 1006, 22330, 2015, 1011, 2053, 1007, 1012, 102], 'labels': [-100, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 0, 0, -100]}, {'input_ids': [101, 1996, 5197, 1997, 2053, 1998, 1996, 4195, 1997, 1052, 2546, 2078, 2487, 1011, 2552, 2378, 15420, 2006, 1996, 7816, 1997, 1052, 2243, 2278, 2001, 2522, 18933, 12821, 4383, 2011, 2058, 10288, 20110, 3258, 1997, 1011, 1162, 14376, 2078, 2487, 1011, 1998, 2552, 2378, 1011, 8031, 28829, 23892, 1997, 1156, 1011, 2552, 2378, 1006, 1039, 24434, 2549, 2015, 1007, 1998, 1052, 2546, 2078, 2487, 1006, 1044, 14526, 2683, 2063, 1007, 1010, 4414, 1010, 2029, 4359, 1996, 5317, 28964, 1997, 1052, 2243, 2278, 2012, 1996, 1011, 1162, 2278, 1011, 15488, 6305, 1012, 102], 'labels': [-100, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, -100]}, {'input_ids': [101, 2122, 9556, 4895, 3726, 4014, 1037, 3117, 2053, 1011, 7790, 7337, 2011, 2029, 1996, 2552, 2378, 22330, 13122, 11705, 18903, 2078, 7711, 1996, 3029, 1998, 13791, 1997, 14828, 12702, 20464, 19966, 2545, 2012, 1996, 2003, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -100]}, {'input_ids': [101, 2119, 2033, 19658, 2509, 23616, 2080, 1998, 2037, 3748, 1011, 2828, 1006, 1059, 2102, 1007, 19070, 15416, 2020, 12599, 2012, 1041, 16048, 1012, 1019, 2007, 1019, 1011, 22953, 5302, 1011, 1016, 1521, 1011, 2139, 11636, 10513, 14615, 3170, 1006, 7987, 8566, 1007, 2000, 2650, 4442, 14996, 6064, 10752, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 1, 1, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 3522, 2147, 2011, 2149, 1998, 2500, 6083, 2008, 1996, 3677, 1521, 1055, 3684, 5213, 5250, 3938, 1006, 26236, 2361, 21057, 1007, 15775, 4842, 5643, 2064, 16913, 9869, 1996, 12761, 10425, 27797, 2011, 18191, 1031, 2324, 1010, 2539, 1033, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2057, 2864, 1037, 7399, 26237, 2944, 1997, 1996, 3466, 1997, 7473, 2140, 2006, 4487, 29212, 15822, 12612, 1006, 28144, 2072, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 3, 3, 0, 1, 1, 0, 0, -100]}, {'input_ids': [101, 28144, 2072, 2001, 6022, 23900, 2000, 7473, 2140, 7644, 1010, 9104, 2008, 8244, 2007, 2062, 5729, 19637, 2094, 8030, 2036, 2018, 2062, 4487, 29212, 15822, 2004, 7203, 1999, 20965, 1016, 1006, 1054, 2475, 1027, 1014, 1012, 2654, 1010, 1052, 1026, 1014, 1012, 2199, 2487, 1007, 1012, 102], 'labels': [-100, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 16902, 1997, 1006, 1037, 1007, 3446, 1997, 6534, 3742, 6622, 1998, 1006, 1038, 1007, 6534, 3742, 2007, 28699, 18963, 3446, 1006, 1165, 1007, 4358, 2011, 2478, 1054, 1024, 23192, 16478, 1997, 18612, 9031, 1006, 1054, 9739, 2850, 1007, 1999, 1996, 21697, 1011, 2241, 4106, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 4050, 1010, 2012, 2659, 16021, 8197, 16259, 2100, 3893, 2250, 4576, 3778, 1006, 24531, 2361, 1007, 5300, 1010, 2460, 20228, 2497, 2094, 1998, 2659, 20228, 2497, 2361, 2024, 2275, 1010, 2012, 3020, 24531, 2361, 5300, 1010, 2936, 20228, 2497, 2094, 1998, 3020, 20228, 2497, 2361, 2024, 2275, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, -100]}, {'input_ids': [101, 2043, 6195, 1996, 6413, 2791, 1997, 10449, 2019, 9353, 2361, 8778, 2012, 21117, 12458, 1010, 2009, 2001, 10358, 2008, 2045, 2001, 1037, 3906, 15074, 2006, 1996, 3451, 5328, 1997, 3737, 1997, 2331, 1006, 1053, 7716, 1007, 1999, 11959, 2021, 2048, 3025, 2913, 2013, 2642, 11959, 2018, 5393, 2008, 1053, 7716, 2001, 2019, 4493, 4145, 1999, 2642, 11959, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 3893, 4442, 2013, 2169, 11375, 2020, 8897, 2013, 1037, 6263, 1997, 2176, 18154, 3479, 4249, 2566, 2152, 2373, 2492, 1006, 6522, 2546, 1007, 1998, 4102, 2007, 1996, 2491, 2011, 20177, 1996, 2779, 2193, 1997, 4442, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1042, 11020, 1010, 26899, 5806, 16902, 1025, 2720, 2078, 2615, 1010, 1049, 1012, 21069, 6137, 7293, 18891, 7946, 1025, 7605, 1010, 2093, 1011, 8789, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 3, 0, -100]}, {'input_ids': [101, 1999, 8469, 3372, 5510, 4442, 1010, 3670, 1997, 22597, 1998, 1996, 22597, 10769, 24312, 1155, 1006, 20868, 14608, 1007, 2038, 2042, 3818, 1031, 1015, 1033, 1010, 1031, 1016, 1033, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2000, 3231, 2023, 1010, 2057, 6086, 1996, 8000, 20014, 4355, 3170, 2000, 2048, 3785, 1024, 1015, 1007, 2382, 8117, 1997, 2003, 5403, 10092, 2628, 2011, 2382, 8117, 1997, 16360, 2121, 20523, 6911, 1010, 1998, 1016, 1007, 2003, 5403, 10092, 1013, 16360, 2121, 20523, 1006, 1045, 1013, 1054, 1007, 6911, 11211, 2007, 11320, 22311, 2140, 1999, 10085, 9513, 2007, 1052, 1012, 29347, 26549, 5740, 3736, 1006, 1045, 1009, 6643, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1996, 2334, 3989, 1997, 9587, 18724, 15420, 1010, 1998, 8558, 1996, 3257, 1997, 2929, 1010, 2003, 4340, 2011, 2019, 9808, 6895, 4571, 7062, 2291, 1010, 2029, 2950, 8203, 20292, 9587, 18724, 1037, 1006, 11460, 2721, 1007, 1010, 1037, 2235, 20710, 1011, 2066, 14181, 19707, 2063, 1010, 1998, 8203, 20292, 9587, 18724, 1038, 1006, 11460, 20850, 1007, 1010, 2049, 14181, 19707, 2063, 2552, 17441, 5250, 1006, 6578, 1007, 2004, 2350, 6177, 1031, 1015, 1010, 1018, 1010, 1019, 1033, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 2, 2, 2, 3, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2478, 5072, 2892, 1011, 11383, 1013, 5796, 1010, 2057, 2018, 2042, 2583, 2000, 8080, 23758, 3370, 2389, 3431, 1999, 1996, 27854, 1011, 8031, 5884, 1006, 6053, 2094, 1007, 1997, 4903, 2906, 1011, 1155, 2588, 27854, 8031, 1012, 1031, 1022, 1033, 1999, 13578, 2094, 2240, 1048, 2620, 2001, 2055, 1996, 2087, 6802, 2240, 2000, 1996, 2093, 3231, 2545, 2241, 2006, 2119, 1043, 5104, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100]}, {'input_ids': [101, 2004, 3491, 1999, 20965, 1015, 1010, 1996, 7403, 3292, 10035, 2090, 1996, 2358, 3089, 3654, 13070, 3210, 1998, 1996, 13070, 3231, 2121, 2020, 12368, 1999, 1996, 2846, 1997, 1043, 2094, 5300, 2084, 2216, 2090, 1996, 2358, 3089, 3654, 1011, 13070, 1998, 18002, 3231, 2121, 1010, 8131, 1996, 13070, 3231, 2121, 2001, 2641, 2004, 1037, 7218, 3231, 2121, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 15050, 21733, 10572, 2011, 1038, 1012, 20934, 10623, 11592, 11124, 2003, 7790, 2006, 1996, 8290, 1997, 1996, 11867, 9711, 20318, 2063, 2007, 26632, 21890, 8449, 1998, 2060, 3526, 4127, 1010, 2107, 2004, 23915, 25930, 2102, 1006, 10710, 2102, 1007, 2030, 1056, 4442, 1031, 3486, 1033, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2951, 2024, 5228, 2004, 2812, 1081, 7367, 2213, 1010, 1050, 1027, 1017, 2013, 2012, 2560, 2093, 2981, 7885, 1025, 1050, 2094, 1024, 2025, 11156, 2030, 2917, 1996, 5787, 1997, 24110, 3775, 12516, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1038, 4328, 1024, 2303, 3742, 5950, 1025, 8292, 1024, 16480, 4244, 20902, 2140, 28517, 2099, 1025, 17163, 1024, 4958, 8939, 24587, 14834, 8331, 1025, 10768, 2615, 2487, 1024, 3140, 4654, 8197, 16259, 2100, 3872, 1999, 1015, 2117, 1025, 1042, 25465, 1024, 3140, 8995, 3977, 1025, 6335, 1024, 6970, 2571, 14228, 2078, 1025, 6845, 2050, 1024, 2146, 3772, 8247, 3283, 26942, 2015, 1025, 18832, 1024, 2146, 3772, 14163, 15782, 22612, 2278, 17379, 2015, 1025, 11338, 2361, 1011, 1015, 1024, 26632, 21890, 3351, 18178, 18938, 28804, 5250, 1011, 1015, 1025, 1056, 2290, 1024, 13012, 25643, 17119, 5178, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 2, 3, 3, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 13366, 2546, 1024, 2640, 3466, 1025, 16461, 1024, 26721, 20464, 19966, 2121, 16902, 19064, 1025, 17371, 1024, 3115, 24353, 1025, 26264, 2099, 1024, 6970, 16211, 28228, 2571, 2846, 1012, 102], 'labels': [-100, 1, 1, 0, 2, 3, 0, 1, 0, 2, 2, 2, 2, 3, 3, 0, 1, 0, 2, 3, 0, 1, 1, 0, 2, 2, 2, 2, 3, 0, -100]}, {'input_ids': [101, 1996, 2877, 5320, 1997, 2331, 1998, 22822, 17062, 3012, 4969, 2024, 2512, 9006, 23041, 5555, 3468, 7870, 1006, 13316, 5104, 1007, 1010, 2107, 2004, 11888, 16464, 4295, 1010, 4456, 1010, 14671, 1010, 1998, 1010, 1999, 3327, 1010, 2003, 5403, 7712, 2540, 4295, 1998, 6909, 1031, 1015, 1033, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1996, 9663, 1997, 2492, 4654, 26243, 14049, 8466, 18279, 20746, 4022, 2015, 1006, 10768, 4523, 4523, 1007, 1006, 20965, 1020, 2497, 1007, 2001, 7594, 2000, 8080, 19962, 9331, 4588, 13791, 1997, 1996, 6187, 2487, 11918, 2389, 15698, 1012, 102], 'labels': [-100, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 8670, 2480, 1010, 1038, 4328, 1011, 1038, 4328, 2005, 1011, 2287, 1062, 1011, 3556, 1025, 1056, 1010, 13012, 8625, 1011, 2000, 1011, 8915, 6494, 8625, 2122, 3565, 9006, 19386, 2229, 2024, 8924, 1997, 23060, 8524, 3366, 2007, 2593, 16464, 3375, 28954, 1998, 22330, 3406, 20366, 1039, 24087, 2475, 1006, 8040, 1024, 3523, 2549, 12848, 2475, 1010, 24829, 1024, 3523, 2549, 12848, 2549, 1007, 2030, 1997, 3375, 1010, 3523, 9006, 19386, 2072, 1998, 22330, 2102, 1006, 7842, 1024, 1045, 2487, 28954, 2549, 12848, 2549, 1025, 1055, 2487, 1010, 1055, 2509, 1998, 1055, 2549, 20965, 2015, 1007, 1012, 102], 'labels': [-100, 1, 1, 0, 2, 2, 3, 1, 1, 3, 3, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 22498, 2015, 1024, 14931, 1010, 5402, 11207, 1025, 1050, 1013, 1037, 1010, 2025, 12711, 1025, 1050, 2487, 1010, 18906, 2015, 1011, 18906, 2015, 2522, 2615, 1011, 1016, 16371, 14321, 24755, 4523, 3593, 4962, 2555, 1015, 1025, 1011, 2522, 2615, 1011, 1016, 10286, 2015, 1010, 5729, 11325, 16464, 8715, 21887, 23350, 1016, 1025, 4895, 2243, 1010, 4242, 2000, 2582, 6709, 3278, 16902, 2015, 2090, 6887, 16515, 13874, 2015, 1998, 2175, 8474, 1010, 2057, 2109, 1037, 2175, 2744, 2424, 2121, 4007, 1031, 4749, 1033, 2000, 2522, 14343, 13806, 6887, 16515, 13874, 2015, 2007, 2175, 2478, 1996, 1052, 7011, 2213, 2000, 2175, 12375, 2951, 2013, 1996, 4962, 3031, 6483, 12360, 1012, 102], 'labels': [-100, 0, 0, 0, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 0, -100]}, {'input_ids': [101, 13587, 5423, 3593, 14817, 1006, 18856, 2063, 1999, 11460, 1007, 2001, 4358, 2206, 9226, 22513, 3802, 2632, 1012, 102], 'labels': [-100, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1006, 10476, 1007, 1998, 5423, 3593, 4180, 1006, 29215, 1007, 2001, 4358, 1999, 11460, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1048, 1011, 1015, 1012, 102], 'labels': [-100, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2028, 2154, 2044, 1996, 7547, 1010, 14704, 8578, 2020, 9099, 28901, 2007, 16298, 2080, 1011, 3378, 7865, 1006, 9779, 2615, 1007, 14262, 26305, 1023, 9309, 17181, 11503, 3211, 2072, 1012, 1045, 23296, 2226, 2226, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1059, 28139, 1011, 1044, 5603, 1031, 2382, 1033, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 14557, 20647, 13181, 3366, 12943, 2906, 1006, 22851, 2050, 1007, 1998, 14557, 20647, 13181, 3366, 22953, 2705, 1006, 22851, 2497, 1007, 2020, 4156, 2013, 4487, 11329, 2080, 1006, 1038, 2094, 1010, 3915, 1007, 1012, 102], 'labels': [-100, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 13853, 26427, 1006, 6187, 3597, 2509, 1007, 1998, 22886, 2020, 4156, 2013, 25353, 6238, 2213, 1006, 28904, 1010, 6027, 1007, 1012, 102], 'labels': [-100, 2, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 4358, 1043, 21297, 2121, 7934, 10882, 7096, 8156, 6165, 1006, 1041, 25708, 2869, 1007, 2020, 10174, 2478, 1996, 2176, 1011, 8023, 8522, 5173, 2013, 1996, 14080, 1997, 8738, 1999, 25125, 4295, 1006, 9108, 4103, 1007, 2817, 1012, 102], 'labels': [-100, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, -100]}, {'input_ids': [101, 2057, 2109, 1996, 2892, 1011, 27197, 2951, 2164, 1020, 1010, 8148, 4587, 2214, 1006, 4793, 3770, 2086, 2214, 2030, 3080, 1007, 2013, 1996, 7403, 4942, 3367, 20217, 1997, 1996, 2822, 20134, 7965, 26906, 5002, 1006, 18856, 7317, 2015, 1007, 2029, 2003, 1037, 2120, 2898, 2522, 27794, 2817, 2008, 2211, 1999, 2687, 2007, 3582, 1011, 2039, 12265, 2296, 1016, 1516, 1017, 2086, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 10889, 1010, 2348, 6335, 1011, 2570, 20544, 2015, 4742, 9099, 8566, 17119, 1998, 2552, 11444, 4263, 1997, 14193, 1017, 1006, 28093, 2509, 1007, 1999, 9706, 2278, 1011, 15527, 4442, 1010, 28093, 2509, 4539, 9165, 2020, 2025, 10572, 1012, 102], 'labels': [-100, 0, 0, 0, 1, 1, 1, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1044, 2232, 1010, 17834, 25852, 1025, 17212, 2546, 2475, 1010, 4517, 5387, 1011, 9413, 22123, 8093, 9314, 1016, 1011, 2066, 1016, 10047, 19797, 2509, 1010, 5110, 19960, 22187, 2854, 9334, 23245, 1025, 1050, 2497, 1010, 28991, 23684, 1025, 15488, 2080, 1010, 5744, 6675, 1012, 102], 'labels': [-100, 1, 1, 0, 2, 2, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 4919, 1010, 1996, 2512, 1011, 16731, 2278, 6818, 2443, 13138, 2007, 3806, 13181, 18447, 19126, 10722, 20360, 1006, 21025, 2102, 1007, 1010, 6421, 2007, 28378, 11290, 10722, 20360, 1006, 1038, 7096, 1007, 1010, 3429, 2509, 2007, 11290, 25022, 12171, 25229, 1010, 2676, 2007, 11888, 28389, 1039, 1006, 10381, 2278, 1007, 1010, 6564, 2007, 11888, 28389, 1038, 1006, 10381, 2497, 1007, 1010, 2753, 2007, 28378, 3806, 13181, 18447, 19126, 4295, 1006, 1038, 5856, 2094, 1007, 1998, 21679, 7965, 2111, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2000, 9699, 19701, 2035, 26741, 1010, 2057, 4912, 2000, 3443, 23892, 2007, 2312, 15778, 3972, 20624, 5644, 2011, 12697, 1016, 2309, 5009, 12987, 2015, 1006, 22214, 12789, 2015, 1007, 2306, 2030, 24958, 1996, 7872, 1011, 7077, 2555, 1006, 20965, 26314, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2478, 7473, 2099, 8991, 4140, 22571, 2075, 1010, 2057, 5147, 4453, 4264, 4755, 14494, 1999, 2296, 14719, 16576, 2475, 2155, 2266, 1999, 1996, 1056, 2487, 4245, 1006, 20965, 26314, 1025, 1055, 2487, 20965, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 14246, 1010, 1041, 5092, 2615, 11255, 1041, 5092, 2615, 1043, 2135, 3597, 21572, 9589, 1012, 102], 'labels': [-100, 1, 0, 2, 2, 2, 3, 1, 1, 1, 3, 3, 3, 3, 3, 0, -100]}, {'input_ids': [101, 2057, 2109, 1037, 3563, 27781, 2114, 1996, 20877, 6672, 4135, 5666, 4588, 25468, 1006, 7610, 2140, 1007, 5250, 2000, 2817, 1996, 6580, 1997, 1996, 7610, 2140, 4517, 4230, 1010, 2029, 2024, 2092, 1011, 7356, 4942, 11231, 14321, 2906, 5090, 1031, 2570, 1033, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1999, 5903, 1010, 1996, 3446, 1997, 11604, 11219, 2930, 1006, 1039, 1011, 2930, 1007, 23534, 2038, 2036, 3445, 12099, 2058, 1996, 2197, 5109, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 2, 2, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1528, 1054, 4877, 11616, 2001, 2081, 2006, 1996, 16474, 9181, 1997, 1996, 2248, 15035, 3456, 8715, 2817, 2177, 1006, 2230, 1007, 1012, 102], 'labels': [-100, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 26264, 2099, 1024, 6970, 16211, 28228, 2571, 2846, 6660, 1010, 7667, 1997, 2060, 20155, 10857, 2003, 6827, 1999, 3653, 1011, 1998, 2695, 1011, 2771, 3785, 2029, 3952, 2950, 1010, 12123, 18423, 3798, 1010, 2303, 3742, 5950, 1006, 1038, 4328, 1007, 1010, 2152, 4304, 5423, 3593, 1006, 10751, 2140, 1007, 1010, 2659, 4304, 5423, 3593, 1006, 25510, 2140, 1007, 1998, 13012, 25643, 17119, 8621, 3798, 1012, 102], 'labels': [-100, 1, 1, 0, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 4840, 9980, 6731, 1999, 1014, 1012, 1015, 1003, 17712, 15185, 2020, 9358, 3089, 11263, 5999, 2005, 2321, 8117, 2012, 5385, 11575, 1998, 3202, 6731, 1999, 1016, 19968, 1997, 4241, 20850, 8586, 3597, 1005, 1055, 6310, 6755, 1005, 1055, 1006, 1040, 4168, 2213, 1007, 1009, 18667, 2050, 2692, 1012, 1015, 1003, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1999, 2804, 1010, 8915, 2213, 3896, 2006, 29215, 2509, 1011, 2462, 3798, 2020, 2036, 6560, 1999, 1019, 1005, 16298, 20049, 2638, 18847, 8458, 2891, 24556, 1011, 8878, 5250, 21903, 1006, 23713, 2243, 1007, 1155, 2487, 1013, 1155, 2475, 3313, 12849, 2033, 10343, 1006, 1055, 2575, 2497, 20965, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 11888, 1044, 2497, 2615, 8985, 2001, 6219, 2429, 2000, 1996, 6745, 2647, 2523, 2005, 1996, 2817, 1997, 1996, 11290, 1006, 19413, 14540, 1007, 6612, 3218, 11594, 1006, 18133, 2290, 1007, 1031, 1015, 1033, 1012, 102], 'labels': [-100, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 22498, 2015, 1024, 25022, 1010, 7023, 13483, 1025, 26419, 2072, 1010, 19686, 4167, 4544, 1012, 102], 'labels': [-100, 0, 0, 0, 1, 0, 2, 3, 0, 1, 1, 0, 2, 3, 3, 0, -100]}, {'input_ids': [101, 2023, 2817, 2003, 2988, 2004, 2566, 1996, 10495, 4781, 1997, 7316, 7012, 1006, 13440, 1007, 1006, 1055, 2487, 13440, 4638, 9863, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1996, 3891, 1997, 8909, 7457, 2076, 10032, 2349, 2000, 2019, 3623, 1999, 11062, 3707, 5918, 2000, 8752, 1996, 4935, 1997, 11062, 2417, 2668, 3526, 1006, 21144, 2278, 1007, 3742, 1010, 2458, 1997, 1996, 2173, 12380, 1998, 10768, 5809, 1010, 1998, 1996, 3279, 1997, 2668, 3378, 2007, 4450, 1998, 6959, 1031, 1016, 1033, 1012, 102], 'labels': [-100, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2023, 2817, 2003, 2988, 2429, 2000, 1996, 16003, 1996, 7316, 1997, 8089, 2389, 2913, 1999, 4958, 5178, 4328, 6779, 1006, 2358, 3217, 4783, 1007, 4861, 1006, 1055, 2487, 2358, 3217, 4783, 4638, 9863, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, -100]}, {'input_ids': [101, 8040, 28433, 4588, 3778, 7774, 1997, 1037, 12170, 4502, 2361, 19536, 1006, 1037, 1007, 4102, 2000, 19536, 1999, 20228, 2497, 2615, 5549, 1006, 1038, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 12170, 4502, 2361, 1024, 23974, 15985, 3893, 2250, 4576, 3778, 1025, 24531, 2361, 1024, 16021, 8197, 16259, 2100, 3893, 2250, 4576, 3778, 1025, 19044, 2361, 1024, 4654, 8197, 16259, 2100, 3893, 2250, 4576, 3778, 1025, 20228, 2497, 2615, 1024, 21954, 2970, 5505, 19536, 1012, 102], 'labels': [-100, 1, 1, 1, 0, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 1, 0, 2, 3, 3, 3, 0, -100]}, {'input_ids': [101, 13365, 13853, 3863, 2869, 1006, 13316, 2595, 1007, 2024, 2659, 16730, 2152, 3977, 1006, 21117, 4017, 2456, 1516, 13509, 1055, 27944, 1007, 3424, 6442, 2545, 2008, 2064, 5901, 2693, 6187, 2475, 1009, 15956, 2408, 1996, 12123, 10804, 1012, 102], 'labels': [-100, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 4442, 2020, 2187, 2000, 7818, 2005, 1017, 1516, 1018, 5486, 2420, 2000, 2433, 1037, 9530, 10258, 24997, 2102, 18847, 24314, 1006, 4983, 3090, 4728, 1007, 2077, 8081, 3370, 2007, 1018, 1003, 11498, 14192, 19058, 11106, 18124, 1006, 1052, 7011, 1007, 2001, 2864, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 1, 0, 0, 0, 0, -100]}, {'input_ids': [101, 9308, 1010, 7632, 2546, 1011, 1016, 14608, 1516, 7790, 2039, 1011, 7816, 1997, 6970, 2571, 14228, 2078, 1006, 6335, 1007, 1011, 1020, 1999, 13109, 2015, 25194, 20582, 1997, 16215, 16576, 4442, 1517, 10232, 3466, 5668, 1997, 10958, 26835, 19009, 1012, 102], 'labels': [-100, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -100]}, {'input_ids': [101, 2057, 7718, 1016, 2659, 1011, 8015, 4617, 1997, 17324, 2102, 1999, 1037, 3979, 1006, 8546, 2969, 1011, 2393, 1998, 3569, 3274, 5084, 10699, 1011, 9164, 7242, 1031, 10507, 19279, 1033, 1007, 1010, 5604, 2037, 4254, 2006, 5022, 2007, 1051, 19797, 2043, 3024, 3188, 2000, 17324, 2102, 2007, 1037, 19294, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1996, 21877, 2140, 7903, 15824, 15099, 1011, 15219, 6705, 1006, 9706, 2094, 1007, 1010, 2036, 2170, 1996, 27885, 13473, 12412, 9530, 9103, 5867, 1010, 2001, 7594, 2013, 1996, 15099, 2522, 28228, 9289, 3302, 1997, 1996, 17266, 17597, 19731, 10024, 2140, 4850, 1006, 19430, 13663, 2854, 1007, 2000, 1996, 7541, 2391, 2006, 1996, 18309, 15219, 1011, 6020, 7814, 1997, 1996, 9047, 2483, 25353, 8737, 10536, 6190, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2011, 5688, 1010, 9412, 2665, 1011, 10077, 5250, 1011, 20446, 8978, 11483, 1006, 1041, 25708, 2361, 1011, 12395, 1007, 2003, 1037, 2172, 16176, 1010, 28991, 5302, 8017, 16730, 3283, 26942, 1997, 1996, 10442, 1038, 2475, 2099, 2008, 2515, 2025, 14187, 2000, 9078, 1031, 1023, 1033, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2747, 2045, 2003, 2053, 3115, 2117, 1011, 2240, 3949, 2005, 26261, 22943, 1011, 25416, 22648, 7062, 12943, 2615, 14945, 1006, 5034, 1011, 12943, 2615, 14945, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 3, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, -100]}, {'input_ids': [101, 3365, 9942, 2000, 7438, 5034, 1011, 12943, 2615, 14945, 2031, 2042, 2988, 1010, 2021, 3463, 2031, 2042, 15640, 2007, 9671, 3433, 6165, 1998, 2146, 2744, 3452, 7691, 1006, 9808, 1007, 1997, 2069, 2322, 1516, 2382, 1003, 1031, 1016, 1033, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 6315, 2951, 2005, 9320, 1037, 1998, 1038, 2064, 2022, 2179, 1999, 1996, 4637, 2592, 2930, 1055, 2475, 2951, 5371, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 4718, 14277, 1010, 1050, 1011, 2433, 8516, 11368, 4048, 16585, 2140, 1011, 3393, 14194, 8516, 1011, 6887, 2368, 23943, 21141, 2638, 1025, 1047, 2094, 1010, 7324, 7698, 1025, 24978, 4095, 12789, 1010, 2512, 13102, 8586, 18513, 14021, 12789, 14021, 12789, 1012, 102], 'labels': [-100, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 2, 0, 1, 1, 1, 0, 2, 2, 2, 2, 3, 3, 1, 1, 0, -100]}, {'input_ids': [101, 2000, 5646, 3251, 2023, 20187, 3433, 2003, 7686, 2011, 2039, 1011, 7816, 1997, 4013, 1011, 20187, 16387, 1010, 2057, 2864, 7901, 14193, 7473, 2099, 1006, 19387, 1011, 7473, 2099, 1007, 2006, 9673, 4755, 1037, 8590, 2175, 3468, 2102, 3526, 10859, 1010, 1998, 5262, 2057, 2156, 3445, 3798, 1997, 28286, 2546, 14608, 1010, 1039, 2595, 20464, 2487, 1010, 6335, 2620, 1010, 2065, 2078, 2487, 1010, 1998, 6335, 2487, 29720, 1999, 2122, 9673, 1006, 3275, 1055, 2487, 2050, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 9308, 1010, 2065, 2057, 2298, 2011, 20155, 7473, 2099, 1006, 1053, 15042, 2099, 1007, 2012, 3798, 1997, 2048, 1997, 2122, 9165, 1010, 6335, 2487, 29720, 1998, 1039, 2595, 20464, 2487, 1010, 2057, 2156, 2037, 3154, 15946, 1999, 9673, 2073, 1058, 12521, 8180, 3670, 2003, 10572, 2011, 3684, 5213, 2069, 1020, 1044, 2077, 12987, 14676, 1010, 9104, 2008, 4013, 1011, 20187, 4962, 3670, 2003, 1037, 5915, 3433, 2000, 1058, 12521, 8180, 3670, 1006, 3275, 1055, 2487, 2497, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 6419, 1050, 16118, 7315, 2011, 2079, 7630, 2618, 17643, 21663, 1006, 26718, 2290, 1007, 2089, 13730, 1996, 3659, 1997, 5012, 1010, 2021, 2009, 2003, 10599, 2129, 4621, 2023, 6942, 2097, 2022, 1998, 2029, 5776, 2967, 2323, 2022, 7237, 2013, 1050, 16118, 3775, 2000, 26718, 2290, 1012, 102], 'labels': [-100, 0, 1, 1, 1, 0, 2, 2, 2, 2, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, -100]}, {'input_ids': [101, 24051, 8462, 24558, 4106, 11156, 10114, 11658, 2135, 5228, 9165, 1006, 2139, 5620, 1007, 3141, 2000, 23060, 8524, 6024, 6911, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2633, 1010, 15982, 2592, 2001, 5067, 1010, 2164, 1996, 6377, 11778, 2729, 2005, 3080, 2111, 1006, 2003, 16186, 1007, 3160, 20589, 1010, 2000, 5646, 3375, 2740, 3471, 1999, 1996, 6818, 1031, 3486, 1033, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1996, 7975, 3544, 2000, 3768, 1996, 2030, 2705, 12898, 2290, 2005, 1996, 2529, 11721, 3676, 19892, 1006, 13091, 13096, 8569, 3723, 7277, 5648, 1516, 8031, 10769, 1007, 1015, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, -100]}, {'input_ids': [101, 12328, 1022, 1516, 2184, 3134, 2214, 10372, 26721, 4842, 9956, 22084, 9215, 2007, 9402, 1006, 1037, 1004, 1041, 1007, 1010, 9800, 1006, 1038, 1004, 1042, 1007, 1010, 9645, 1006, 1039, 1004, 1043, 1007, 1010, 2030, 10114, 1006, 1040, 1004, 1044, 1007, 11952, 5716, 3197, 1006, 1052, 11263, 1007, 3044, 10178, 11338, 2213, 2615, 2018, 15965, 2668, 18847, 11231, 14321, 2906, 4442, 1006, 1052, 25526, 2278, 1007, 16330, 2385, 3134, 2044, 8985, 2005, 14200, 5649, 11338, 2213, 2615, 1011, 3563, 1056, 1011, 4442, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 7561, 6963, 2020, 5020, 2000, 1996, 3115, 7561, 1997, 1996, 2812, 1006, 7367, 2213, 1007, 1997, 1037, 16745, 1997, 1996, 2334, 21318, 2015, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100]}, {'input_ids': [101, 1996, 16745, 2001, 4217, 2061, 2008, 2053, 2048, 27189, 4207, 2019, 3341, 1010, 12725, 2008, 2334, 16902, 2015, 1999, 1996, 21318, 2106, 2025, 7976, 2135, 5547, 1996, 7367, 2213, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100]}, {'input_ids': [101, 1996, 7367, 2213, 2001, 9398, 4383, 2478, 1037, 6879, 6494, 2361, 12504, 1012, 102], 'labels': [-100, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2057, 2424, 2008, 23996, 14996, 8394, 2331, 8176, 20275, 3653, 4890, 13490, 1997, 8171, 3223, 2005, 5292, 6633, 8649, 4135, 8428, 2039, 15166, 1998, 1996, 16012, 23737, 1997, 1996, 17886, 3512, 12436, 10841, 9890, 1006, 1040, 2615, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, -100]}, {'input_ids': [101, 2057, 2582, 10580, 28829, 2334, 6648, 1997, 3653, 4890, 13776, 11626, 2865, 6591, 1010, 28829, 2039, 15166, 1997, 5292, 6633, 8649, 4135, 8428, 1010, 1998, 1037, 20275, 1040, 2615, 1999, 2122, 23996, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, -100]}, {'input_ids': [101, 15877, 1006, 26419, 1007, 2003, 2028, 1997, 1996, 2877, 5320, 1997, 2331, 2013, 16514, 4295, 4969, 1010, 1998, 2009, 8563, 2105, 1015, 1012, 1021, 2454, 2111, 2169, 2095, 1012, 102], 'labels': [-100, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1999, 4766, 1010, 1037, 11038, 2433, 1997, 19802, 6190, 1011, 3141, 5812, 4945, 7667, 1006, 10682, 1007, 3556, 1031, 2603, 1033, 1011, 2241, 4577, 2291, 1010, 1996, 18856, 10128, 1011, 1039, 1997, 3556, 1010, 2001, 2109, 2000, 16157, 1997, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, -100]}, {'input_ids': [101, 1996, 18856, 10128, 1011, 6187, 20464, 2546, 3556, 2001, 2764, 2011, 11566, 1996, 18856, 10128, 1011, 2522, 2546, 3556, 1998, 2048, 2060, 2981, 4013, 26745, 10074, 5876, 1010, 2287, 1010, 1998, 2317, 2668, 3526, 1006, 25610, 2278, 1007, 4175, 1031, 2570, 1033, 1012, 102], 'labels': [-100, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1996, 2093, 1011, 16381, 2944, 2008, 2106, 2025, 13265, 6970, 1011, 8875, 24004, 6914, 20175, 2100, 1006, 2004, 3491, 1999, 3275, 1022, 2050, 1007, 14729, 2005, 2069, 4868, 1003, 1997, 1996, 23284, 1006, 1054, 1016, 1007, 1010, 1998, 2018, 2019, 9875, 17339, 2592, 19229, 1006, 9932, 2278, 1007, 3643, 1006, 2156, 4725, 1007, 1997, 1015, 1010, 5890, 2692, 1006, 3020, 9932, 2278, 5300, 5769, 4788, 4906, 1031, 2676, 1033, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1040, 17134, 1010, 16510, 27710, 5402, 2410, 28086, 2015, 1025, 6574, 2475, 1010, 2130, 16791, 18247, 1016, 1025, 25188, 1010, 10424, 27225, 14117, 2102, 1025, 1044, 16932, 1010, 5292, 24759, 9314, 5402, 2403, 28086, 2015, 1025, 14161, 2072, 1010, 14161, 4313, 4523, 1025, 1050, 1010, 6463, 1997, 23767, 2000, 22330, 14399, 8523, 2213, 1025, 7473, 2532, 1010, 4013, 15509, 15172, 3526, 4517, 28873, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 3, 3, 3, 0, 1, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 0, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2057, 14925, 14399, 15004, 5228, 1050, 1012, 13675, 12054, 2050, 16405, 2546, 2509, 5250, 19660, 2000, 1037, 18231, 16730, 28406, 6415, 1006, 11112, 1011, 6415, 1007, 1999, 1037, 1055, 1012, 8292, 2890, 11365, 19001, 10178, 4394, 2203, 23924, 3560, 16405, 2546, 8171, 1010, 16405, 2546, 2487, 1011, 1019, 1006, 5173, 2013, 1019, 29722, 14289, 10343, 10178, 1031, 4700, 1033, 1007, 1010, 1998, 2057, 4453, 1996, 12987, 2015, 5391, 2011, 1050, 1012, 13675, 12054, 2050, 16405, 2546, 2509, 1006, 4475, 1998, 4725, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2023, 15861, 3012, 2001, 10358, 2043, 2057, 13115, 2035, 16861, 4655, 2000, 1996, 2034, 16371, 14321, 19137, 4168, 13248, 1997, 1996, 24529, 2015, 1998, 4396, 2035, 2122, 9165, 2011, 1037, 16371, 14321, 19137, 4168, 19120, 15861, 3012, 1006, 27937, 2361, 1007, 3556, 4162, 2000, 1996, 16861, 2555, 1006, 3275, 23842, 1025, 4475, 1998, 4725, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 3568, 1010, 1996, 2817, 6461, 2000, 6592, 1996, 5072, 5512, 1997, 1039, 1012, 27885, 5809, 2050, 7053, 14817, 2478, 29215, 1011, 5796, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2000, 14358, 2065, 1010, 4661, 1996, 3424, 11636, 8524, 3372, 4023, 1010, 1039, 1012, 27885, 5809, 2050, 2052, 8327, 2019, 3424, 12520, 3466, 1998, 2029, 10595, 2052, 2022, 2920, 1999, 2009, 1010, 2057, 16330, 1996, 3466, 1997, 1996, 14817, 2006, 17710, 8609, 5740, 27321, 1998, 10882, 12618, 28522, 12837, 6086, 2000, 23068, 8249, 1998, 16578, 5250, 6351, 8516, 1010, 8902, 20679, 2078, 1010, 1044, 21095, 21017, 2594, 5648, 1006, 5292, 1007, 1010, 3384, 4135, 21572, 9589, 11022, 1011, 1015, 1006, 3461, 2361, 1011, 1015, 1007, 1010, 6335, 1011, 1015, 1156, 1998, 6335, 1011, 1020, 8417, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2057, 2034, 16578, 5966, 2090, 11682, 2006, 1996, 4023, 7236, 1024, 3622, 5776, 2729, 1006, 1040, 15042, 1007, 1010, 7268, 5776, 2729, 1006, 28569, 1007, 1010, 2236, 8518, 1006, 14181, 1007, 1998, 2060, 8518, 1006, 27178, 1007, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 2, 3, 3, 0, 1, 0, 0, 2, 3, 0, 1, 0, 0, 2, 3, 0, 1, 0, 0, -100]}, {'input_ids': [101, 5512, 2389, 4106, 4487, 25572, 4570, 2008, 2028, 8023, 3791, 2000, 2022, 4217, 2004, 1037, 4431, 8023, 1010, 2000, 12826, 1996, 2500, 2114, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 4919, 1010, 2057, 8209, 2128, 13770, 2015, 2478, 1996, 9078, 3406, 18037, 11368, 29598, 28517, 2099, 1006, 2572, 1011, 28517, 2099, 1007, 1997, 2023, 18554, 1006, 2156, 4725, 1007, 1010, 1998, 2059, 2057, 27846, 4487, 24137, 7405, 3064, 3265, 8473, 6302, 2890, 3401, 13876, 5668, 1998, 1013, 2030, 9808, 2015, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100]}, {'input_ids': [101, 2057, 7021, 1996, 7547, 2478, 20868, 21203, 2012, 9683, 13221, 1998, 2019, 20868, 1011, 7591, 2678, 4950, 4987, 2000, 1996, 24635, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 2005, 2070, 2086, 1010, 1996, 4555, 23370, 3446, 1006, 2720, 2099, 1007, 1997, 1996, 16021, 8197, 16259, 2100, 6650, 2038, 2042, 2109, 2004, 2019, 14958, 12115, 1997, 16342, 1998, 2058, 11066, 1997, 1996, 16021, 8197, 16259, 2100, 6650, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 9327, 3802, 2632, 1012, 102], 'labels': [-100, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1031, 2385, 1033, 7645, 2008, 14021, 2361, 2475, 7711, 6887, 2891, 8458, 10253, 13490, 1998, 13791, 1997, 5034, 2278, 2155, 21903, 2015, 1006, 16420, 5705, 1007, 1010, 1999, 2008, 16420, 2243, 4023, 2003, 4359, 1999, 1996, 6438, 1997, 14021, 2361, 2475, 1010, 2349, 2000, 23760, 8458, 2891, 8458, 10253, 13490, 1997, 1996, 24054, 2100, 6887, 2891, 8458, 10253, 13490, 2609, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 14089, 1010, 15053, 23058, 2099, 5516, 16226, 1025, 11867, 15671, 2683, 1010, 2358, 2890, 13876, 24163, 27631, 1052, 7677, 6914, 2229, 25222, 2683, 1025, 1059, 2102, 1010, 3748, 1011, 2828, 1012, 102], 'labels': [-100, 1, 0, 2, 2, 2, 3, 3, 0, 1, 1, 1, 0, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 0, 1, 1, 0, 2, 3, 3, 0, -100]}, {'input_ids': [101, 14405, 13492, 3490, 10415, 5648, 1006, 9779, 1007, 12448, 3370, 26001, 9535, 12604, 9307, 1998, 1040, 2546, 1012, 102], 'labels': [-100, 2, 2, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -100]}, {'input_ids': [101, 1006, 1037, 1010, 1038, 1007, 4297, 19761, 3508, 1997, 14595, 2080, 1011, 1016, 1006, 12987, 2072, 1007, 16253, 1999, 1019, 3461, 9779, 5260, 2000, 9535, 12604, 9307, 19857, 20030, 2714, 2000, 3748, 2828, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}, {'input_ids': [101, 1006, 1037, 1007, 3466, 2006, 9535, 12604, 9307, 19857, 20030, 1997, 4297, 19761, 3508, 1999, 1037, 2846, 1997, 9779, 14061, 1012, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, -100]}, {'input_ids': [101, 1019, 3461, 9779, 3957, 19857, 20030, 3798, 2714, 2000, 3748, 2828, 1012, 102], 'labels': [-100, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}]\n"
     ]
    }
   ],
   "source": [
    "print(tokenised_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenised_train,\n",
    "    eval_dataset=tokenised_val,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 150/150 [05:08<00:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 308.1618, 'train_samples_per_second': 1.947, 'train_steps_per_second': 0.487, 'train_loss': 0.06668521245320638, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.06668521245320638, metrics={'train_runtime': 308.1618, 'train_samples_per_second': 1.947, 'train_steps_per_second': 0.487, 'train_loss': 0.06668521245320638, 'epoch': 6.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 38/39 [00:15<00:00,  2.56it/s]c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "100%|| 39/39 [00:15<00:00,  2.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0]': {'precision': 0.5760869565217391,\n",
       "  'recall': 0.7940074906367042,\n",
       "  'f1': 0.6677165354330709,\n",
       "  'number': 267},\n",
       " '0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0]': {'precision': 0.5187861271676301,\n",
       "  'recall': 0.6697761194029851,\n",
       "  'f1': 0.5846905537459284,\n",
       "  'number': 536},\n",
       " '0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0]': {'precision': 0.4789473684210526,\n",
       "  'recall': 0.610738255033557,\n",
       "  'f1': 0.5368731563421829,\n",
       "  'number': 149},\n",
       " '1, 0, 2, 3, 3, 0]': {'precision': 0.4523809523809524,\n",
       "  'recall': 0.5891472868217055,\n",
       "  'f1': 0.5117845117845118,\n",
       "  'number': 129},\n",
       " 'overall_precision': 0.5204513399153737,\n",
       " 'overall_recall': 0.6827012025901943,\n",
       " 'overall_f1': 0.5906362545018007,\n",
       " 'overall_accuracy': 0.9043752819124944}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenised_test)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
