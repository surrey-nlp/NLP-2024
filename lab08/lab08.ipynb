{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "SGcxT-9jmkYA",
   "metadata": {
    "id": "SGcxT-9jmkYA"
   },
   "source": [
    "## Lab 8: Text generation with GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1yiqnnPsYV-L",
   "metadata": {
    "id": "1yiqnnPsYV-L"
   },
   "source": [
    "During this lab, we will further explore the transformer architecture and GPT. The GPT (Generative Pre-trained Transformer) architecture has significantly advanced the field of NLP by enabling the development of powerful and versatile language models. Its transformer-based design, coupled with unsupervised pre-training on large text corpora, has revolutionized tasks such as text generation, summarization, and language understanding.\n",
    "\n",
    "Even though we are not able to perform a large-scale training in the scope of this lab, we can still explore the capabilities of the model on a smaller scale by training on the `tiny_shakespeare` dataset and utilizing some pre-trained weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b9738a4-3d6a-462b-8fc0-8524f3e2ce76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b9738a4-3d6a-462b-8fc0-8524f3e2ce76",
    "outputId": "ef12d01b-04ad-4415-be2c-e9cc55aa7aa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torcheval\n",
      "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, torcheval, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, nvidia-cusolver-cu12, datasets, bert_score\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed bert_score-0.3.13 datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 tiktoken-0.6.0 torcheval-0.0.7 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch numpy transformers datasets tiktoken tqdm nltk bert_score torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61cd73b-213d-453a-a296-46232cf0b8ec",
   "metadata": {
    "id": "a61cd73b-213d-453a-a296-46232cf0b8ec"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# don't forget to upload model.py into /content\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52faadf3-03a3-4684-bdfc-f0d589e86cc7",
   "metadata": {
    "id": "52faadf3-03a3-4684-bdfc-f0d589e86cc7"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1459705-e77d-47b7-b2e7-d8ffbd107bb9",
   "metadata": {
    "id": "c1459705-e77d-47b7-b2e7-d8ffbd107bb9"
   },
   "source": [
    "Download the `tiny_shakespeare` dataset, which consists of numerous Shakespeare plays concatenated into a single text file.\n",
    "\n",
    "It is encoded with Byte-Pair Encoding (BPE) that builds a vocabulary of subword units to optimally represent the input data. The encoded tokens for each split (train/val) are saved into corresponding binary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456a267-e0e0-418b-ac7f-62ea31ce05bb",
   "metadata": {
    "id": "8456a267-e0e0-418b-ac7f-62ea31ce05bb"
   },
   "outputs": [],
   "source": [
    "input_file_path = os.path.join(os.path.abspath(''), 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2 bpe\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(os.path.abspath(''), 'train.bin'))\n",
    "val_ids.tofile(os.path.join(os.path.abspath(''), 'val.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea99ab-00f8-4172-bb00-68296fcb9c98",
   "metadata": {
    "id": "c3ea99ab-00f8-4172-bb00-68296fcb9c98"
   },
   "source": [
    "### Train a small GPT model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BRGgArgWm6GO",
   "metadata": {
    "id": "BRGgArgWm6GO"
   },
   "source": [
    "Let's train the model for the task of next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97463f9-105c-4495-b0e2-999b7906a775",
   "metadata": {
    "id": "c97463f9-105c-4495-b0e2-999b7906a775"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "\n",
    "init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n",
    "out_dir = 'out-shakespeare'\n",
    "eval_interval = 100\n",
    "eval_iters = 100\n",
    "log_interval = 20\n",
    "always_save_checkpoint = True  # if True, always save a checkpoint after each eval\n",
    "\n",
    "# data\n",
    "dataset = 'shakespeare'\n",
    "gradient_accumulation_steps = 5 * 8  # used to simulate larger batch sizes\n",
    "batch_size = 12  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 128\n",
    "\n",
    "# model\n",
    "# ------------------------------------------------------------------------------\n",
    "# play with these parameters! if you have access to the GPU runtime, make the model\n",
    "# bigger, it has a significant influence on its performance\n",
    "n_layer = 6\n",
    "n_head = 4\n",
    "n_embd = 128\n",
    "# ------------------------------------------------------------------------------\n",
    "dropout = 0.0  # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4  # max learning rate\n",
    "max_iters = 500  # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True  # whether to decay the learning rate\n",
    "warmup_iters = 10  # how many steps to warm up for\n",
    "lr_decay_iters = 500  # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
    "print(f'[.] {device} chosen as device')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys}\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, block_size=block_size)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "\n",
    "def get_batch(split, batch_size=16, block_size=1024):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(os.path.abspath(''), 'train.bin'),\n",
    "                         dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(os.path.abspath(''), 'val.bin'),\n",
    "                         dtype=np.uint16, mode='r')\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i + block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i + 1:i + 1 + block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c639e8-63b6-4bc4-87f0-0f7569922d11",
   "metadata": {
    "id": "f4c639e8-63b6-4bc4-87f0-0f7569922d11"
   },
   "source": [
    "Various inits, derived attributes, I/O setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab55f13-8d25-440c-8894-90090317ce59",
   "metadata": {
    "id": "0ab55f13-8d25-440c-8894-90090317ce59"
   },
   "outputs": [],
   "source": [
    "seed_offset = 0\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'  # for later use in torch.autocast\n",
    "\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "data_dir = os.path.join('data', dataset)\n",
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout)\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "# determine the vocab size we'll use for from-scratch training\n",
    "if meta_vocab_size is None:\n",
    "    print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "\n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size  # so that the checkpoint will have the right value\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None  # free up memory\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)  # requires PyTorch 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b0d9b-e5fb-4a8b-a5e9-24929a4904af",
   "metadata": {
    "id": "099b0d9b-e5fb-4a8b-a5e9-24929a4904af"
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "X, Y = get_batch('train', block_size=block_size)  # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0  # number of iterations in the lifetime of this process\n",
    "\n",
    "while True:\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps  # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train', block_size=block_size)\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt * 1000:.2f}ms\")\n",
    "\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GHjeuVIpXdPJ",
   "metadata": {
    "id": "GHjeuVIpXdPJ"
   },
   "source": [
    "> In case you are having trouble with securing a GPU runtime and training the model, download the trained weights from [here](https://drive.google.com/file/d/17gfJ76SyGJVW3jinz3Xv5B7XxTyE9NNA/view?usp=sharing). Create the directory called `out-shakespeare` and place the downloaded weights there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "uyoXtuGaggRL",
   "metadata": {
    "id": "uyoXtuGaggRL"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('/content/out-shakespeare'):\n",
    "  os.makedirs('/content/out-shakespeare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C8FZY_5Abirw",
   "metadata": {
    "id": "C8FZY_5Abirw"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=17gfJ76SyGJVW3jinz3Xv5B7XxTyE9NNA' -O /content/out-shakespeare/ckpt.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bf31a0-f17a-4103-86d7-e189cc12da5b",
   "metadata": {
    "id": "77bf31a0-f17a-4103-86d7-e189cc12da5b"
   },
   "source": [
    "---\n",
    "### Sample from a trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba4440-2dcc-461a-bf87-75a7607bf389",
   "metadata": {
    "id": "37ba4440-2dcc-461a-bf87-75a7607bf389"
   },
   "source": [
    "Initialize the trained model from a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0edb2-2df8-47d6-8cc9-3b8737baaede",
   "metadata": {
    "id": "a0f0edb2-2df8-47d6-8cc9-3b8737baaede"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> There is no need to re-initialize the model and load it into memory again if you've just trained it. Run the next cell if the context of the notebook was reset after the training or if you downloaded the weights.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a63e56-9ce0-4738-a800-f907b93168a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5a63e56-9ce0-4738-a800-f907b93168a3",
    "outputId": "77505b20-00d9-48c0-a77c-b37094ee3823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 7.62M\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume'\n",
    "out_dir = 'out-shakespeare' # ignored if init_from is not 'resume'\n",
    "\n",
    "seed = 1337\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "\n",
    "# init from a model saved in a specific directory\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b13972-f456-49e4-a59a-afa2d4fb721c",
   "metadata": {
    "id": "23b13972-f456-49e4-a59a-afa2d4fb721c"
   },
   "outputs": [],
   "source": [
    "# assume gpt-2 encodings by default\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78429441-dda3-4ae9-927f-3e00cff0b444",
   "metadata": {
    "id": "78429441-dda3-4ae9-927f-3e00cff0b444"
   },
   "source": [
    "---\n",
    "### Sample from the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fa614-cdb1-49d9-9dfe-64df0d35efd8",
   "metadata": {
    "id": "810fa614-cdb1-49d9-9dfe-64df0d35efd8"
   },
   "source": [
    "We can prompt the model by providing a context. Try sampling with a different context and sample length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4914a-76c6-4554-b5df-f93eff268d32",
   "metadata": {
    "id": "c5b4914a-76c6-4554-b5df-f93eff268d32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encode the beginning of the prompt\n",
    "context = 'The Universe is vast'\n",
    "start_ids = encode(context)\n",
    "num_samples = 5\n",
    "sample_len = 128\n",
    "\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "            probs, y = model.generate(x, sample_len, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('==============================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kkIxQd7JhKTW",
   "metadata": {
    "id": "kkIxQd7JhKTW"
   },
   "source": [
    "The generated text mostly does not make sense but the model could definitely capture some attributes of the Shakespearean style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2334be-c4a4-44b9-a8b4-ab74f61a75a6",
   "metadata": {
    "id": "8c2334be-c4a4-44b9-a8b4-ab74f61a75a6"
   },
   "source": [
    "Let's run the model on the validation dataset. Experiment with the context size and see how it influences the generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37397a23-1d30-492d-9bfb-b56bfae0c1b5",
   "metadata": {
    "id": "37397a23-1d30-492d-9bfb-b56bfae0c1b5"
   },
   "source": [
    "> Note: the context, provided to the model, consists of the first few tokens from each sample. Be shure to exclude is from evaluation as it will always be \"right\" and skew the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdad63a-4755-4629-b58e-c715bc878517",
   "metadata": {
    "id": "3fdad63a-4755-4629-b58e-c715bc878517",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiment with sample length, context size and their influence on the evaluation metrics\n",
    "sample_len = 64\n",
    "batch_size = 32\n",
    "start_len = 5\n",
    "\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "# -------------------------------------\n",
    "\n",
    "val_data = np.memmap('./val.bin', dtype=np.uint16, mode='r')\n",
    "num_batches = len(val_data) // sample_len // batch_size\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for batch_i in range(num_batches):\n",
    "            print(f'batch {batch_i}/{num_batches}')\n",
    "\n",
    "            X_val, _ = get_batch('val', batch_size, sample_len)\n",
    "\n",
    "            for k in range(batch_size):\n",
    "                start_ids = X_val[k, :start_len]\n",
    "                # print('START:', start_ids, ' - \"', decode(list(start_ids)), '\"')\n",
    "                x = start_ids.clone().detach().type(torch.long).to(device)[None, ...]\n",
    "\n",
    "                probs, pred = model.generate(x, sample_len-start_len,\n",
    "                                             temperature=temperature, top_k=top_k)\n",
    "\n",
    "                decoded_pred = decode(pred[0].tolist())\n",
    "                decoded_gt = decode(X_val[k].tolist())\n",
    "\n",
    "                # print('PRED DECODED: ', decoded_pred)\n",
    "                # print('--------------------------')\n",
    "                # print('GT DECODED: ', decoded_gt)\n",
    "                # print('==============================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52098f-3052-428a-84b2-58f1ba01dddd",
   "metadata": {
    "id": "ae52098f-3052-428a-84b2-58f1ba01dddd"
   },
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93d511-64b8-444e-b16b-245b572e7186",
   "metadata": {
    "id": "5e93d511-64b8-444e-b16b-245b572e7186"
   },
   "source": [
    "It is important to be able to quantitatively evaluate language models. Some of the popular evaluation metrics that use reference text are BLEU score, BERTScore and Perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c997b8-605f-450a-a416-b177d09473fc",
   "metadata": {
    "id": "34c997b8-605f-450a-a416-b177d09473fc"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> Save the needed predictions while running the model on the validation dataset in the cell above. Computing the metrics on the dataset level is more straightforward.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec94b8b2-7e63-48e8-a1f3-32f5400a6970",
   "metadata": {
    "id": "ec94b8b2-7e63-48e8-a1f3-32f5400a6970"
   },
   "source": [
    "#### 1. BLEU score\n",
    "\n",
    "The BLEU (Bilingual Evaluation Understudy) score works by comparing the n-grams (contiguous sequences of n tokens) in the generated text to those in the reference text(s). It calculates a precision score for each n-gram size (typically up to 4-grams) and combines these scores using a weighted geometric mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664a9a4-43ed-4e6e-afd9-d7ac3732e673",
   "metadata": {
    "id": "e664a9a4-43ed-4e6e-afd9-d7ac3732e673"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Challenge 1:</b> Use the NLTK framework to calculate BLEU scores for 1-, 2-, 3-, and 4-grams on the validation dataset.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178be33-3e2f-4309-b140-4b8b7f5d017e",
   "metadata": {
    "id": "5178be33-3e2f-4309-b140-4b8b7f5d017e"
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "print('BLEU-1: ', bleu1)\n",
    "print('BLEU-2: ', bleu2)\n",
    "print('BLEU-3: ', bleu3)\n",
    "print('BLEU-4: ', bleu4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d37fa-f010-4ee2-8036-2114b178487c",
   "metadata": {
    "id": "ce5d37fa-f010-4ee2-8036-2114b178487c"
   },
   "source": [
    "#### 2. BERTScore\n",
    "\n",
    "BERTScore  leverages contextual embeddings from BERT (Bidirectional Encoder Representations from Transformers) to compute similarity between sentences or text spans. Unlike BLEU score that works on a token level, it considers both word overlap and contextual information, providing a more accurate evaluation.\n",
    "\n",
    "Compared to BLEU score, BERTScore offers several advantages:\n",
    "\n",
    "- Contextual understanding: BERTScore considers the contextual meaning of words, capturing nuances that BLEU, which relies solely on word overlap, may miss.\n",
    "- Robustness to word order: BERTScore's contextual embeddings enable it to handle variations in word order, making it more robust to changes in sentence structure or word arrangement.\n",
    "- Higher correlation with human judgment: BERTScore has been shown to correlate better with human judgment in evaluating text quality, especially in tasks like summarization and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7609a3-43b8-4956-9175-818b655d5313",
   "metadata": {
    "id": "ba7609a3-43b8-4956-9175-818b655d5313"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Challenge 2:</b> Use the bert_score package to calculate BERTScore on the validation dataset.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f98ed0-3093-4312-8113-38a03a505c78",
   "metadata": {
    "id": "c6f98ed0-3093-4312-8113-38a03a505c78"
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "print(f\"BERTScore Precision: {bs_precision:.4f}, Recall: {bs_recall:.4f}, F1: {bs_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5b48b-beb1-490c-a997-de7610185194",
   "metadata": {
    "id": "5bc5b48b-beb1-490c-a997-de7610185194"
   },
   "source": [
    "#### 3. Perplexity\n",
    "\n",
    "Perplexity is a measurement used in natural language processing (NLP) to assess how well a language model predicts a sample of text. It quantifies the average uncertainty or surprise of the model in predicting the next word or token in a sequence. Lower perplexity values indicate that the model is more confident and accurate in its predictions, while higher values suggest more uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84440291-2467-4294-a041-d448c053b6b5",
   "metadata": {
    "id": "84440291-2467-4294-a041-d448c053b6b5"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Challenge 3:</b> Use the torcheval package to calculate perplexity on the validation dataset.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43c193-a6c9-491d-b86a-123f80928253",
   "metadata": {
    "id": "1d43c193-a6c9-491d-b86a-123f80928253"
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "print('Perplexity: ', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384534b-012f-42a9-9a50-7dc55944754c",
   "metadata": {
    "id": "4384534b-012f-42a9-9a50-7dc55944754c"
   },
   "source": [
    "---\n",
    "Now let's load a pre-trained GPT-2 model and see, how does it perform in terms of the calculated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06326618-1de3-45af-bb19-2cf327fe7755",
   "metadata": {
    "id": "06326618-1de3-45af-bb19-2cf327fe7755"
   },
   "outputs": [],
   "source": [
    "init_from = 'gpt2-medium'  # 'gpt2-xl' if you have access to a decent GPU\n",
    "\n",
    "# init from a given GPT-2 model\n",
    "model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc6e0f-1919-4740-b7c0-cfe32c0f2d69",
   "metadata": {
    "id": "f3fc6e0f-1919-4740-b7c0-cfe32c0f2d69"
   },
   "source": [
    "Sampling the model with a given context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abae0d-8e69-4fd7-9662-c15df300ee29",
   "metadata": {
    "id": "00abae0d-8e69-4fd7-9662-c15df300ee29"
   },
   "outputs": [],
   "source": [
    "# encode the beginning of the prompt\n",
    "context = 'The Universe is vast'\n",
    "start_ids = encode(context)\n",
    "num_samples = 5\n",
    "sample_len = 128\n",
    "\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "            probs, y = model.generate(x, sample_len, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('==============================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e03b7-5206-4131-a6ac-82341c3e4f90",
   "metadata": {
    "id": "934e03b7-5206-4131-a6ac-82341c3e4f90"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Challenge 4:</b> Re-use the code from above to run the model on the validation dataset and calculate BLEU score, BERTScore and perplexity. What do you observe? Do numbers correlate with the qualitative evaluation?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ogRyGe5lJkD",
   "metadata": {
    "id": "6ogRyGe5lJkD"
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ORkIb6rljltv",
   "metadata": {
    "id": "ORkIb6rljltv"
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "print('BLEU-1: ', bleu1)\n",
    "print('BLEU-2: ', bleu2)\n",
    "print('BLEU-3: ', bleu3)\n",
    "print('BLEU-4: ', bleu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vn-9PHhJjtf5",
   "metadata": {
    "id": "Vn-9PHhJjtf5"
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "print(f\"BERTScore Precision: {bs_precision:.4f}, Recall: {bs_recall:.4f}, F1: {bs_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90LrfSP6jtt7",
   "metadata": {
    "id": "90LrfSP6jtt7"
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "print('Perplexity: ', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b02909-e0cd-4f41-89a3-8876b9203efa",
   "metadata": {
    "id": "34b02909-e0cd-4f41-89a3-8876b9203efa"
   },
   "source": [
    "> Do you see drawbacks of the metrics that rely on the reference text? Can we provide an adequate reference in case of an unconstrained text generation? Compare the outputs of both models qualitatively. Think about the other ways to evaluate text generation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mbNwxStCmJMQ",
   "metadata": {
    "id": "mbNwxStCmJMQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
