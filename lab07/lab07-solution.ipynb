{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition using Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will focus on helping you fine-tune Transformers models for Named Entity Recognition. You will use the Datasets library to quickly load and preprocess the WikiNEuRal dataset, getting them ready for training with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Let us first install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers seqeval huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "!sudo apt-get install git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Datasets library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be done with the functions load_dataset and load_metric.\n",
    "\n",
    "The notebook should work with any token classification dataset provided by the Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/usind/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f150d2c24a2425c9cbf23590ad39673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, concatenate_datasets\n",
    "datasets = load_dataset(\"Babelscape/wikineural\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WikiNEuRal dataset has train, validation and test splits for each of the 9 languages.\n",
    "\n",
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['This',\n",
       "  'division',\n",
       "  'also',\n",
       "  'contains',\n",
       "  'the',\n",
       "  'Ventana',\n",
       "  'Wilderness',\n",
       "  ',',\n",
       "  'home',\n",
       "  'to',\n",
       "  'the',\n",
       "  'California',\n",
       "  'condor',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0],\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets[\"train_en\"] refers to the subset of the Wikineural dataset that contains English articles for training. The code datasets[\"train_en\"][0] \n",
    "# retrieves the first article in this subset.\n",
    "\n",
    "datasets[\"train_en\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are already coded as integer ids to be easily usable by our model, but the correspondence with the actual categories is stored the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first line defines a list of NER label strings,\n",
    "# where each label represents a different named entity category, \n",
    "# such as a person (PER), organization (ORG), location (LOC), or miscellaneous (MISC).\n",
    "\n",
    "# B (Beginning) indicates the first token of a named entity.\n",
    "# I (Inside) indicates any non-first token of a named entity.\n",
    "# O (Outside) indicates that a token is not part of a named entity.\n",
    "\n",
    "label_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "#creates a dictionary called labels_vocab where each label string is a key and the corresponding \n",
    "#value is an integer that represents that label.\n",
    "\n",
    "labels_vocab = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "\n",
    "#labels_vocab_reverse dictionary is used to map the integer labels back to their string representations. \n",
    "labels_vocab_reverse = {v:k for k,v in labels_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train a monolingual model (NER on a single language), you have to use a single language for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets[\"train_en\"]\n",
    "val_dataset = datasets[\"val_en\"]\n",
    "test_dataset = datasets[\"test_en\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you can concatenate multiple dataset splits to train a Multilingual NER model. However, let us focus on Monolingual NER for now. You can uncomment and use the following for other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = concatenate_datasets([datasets[\"train_de\"], \n",
    "#                                       datasets[\"train_en\"], \n",
    "#                                       datasets[\"train_es\"], \n",
    "#                                       datasets[\"train_fr\"],\n",
    "#                                       datasets[\"train_it\"],\n",
    "#                                       datasets[\"train_nl\"],\n",
    "#                                       datasets[\"train_pl\"],\n",
    "#                                       datasets[\"train_pt\"],\n",
    "#                                       datasets[\"train_ru\"]])\n",
    "\n",
    "# val_dataset = concatenate_datasets([datasets[\"val_de\"], \n",
    "#                                     datasets[\"val_en\"], \n",
    "#                                     datasets[\"val_es\"], \n",
    "#                                     datasets[\"val_fr\"],\n",
    "#                                     datasets[\"val_it\"],\n",
    "#                                     datasets[\"val_nl\"],\n",
    "#                                     datasets[\"val_pl\"],\n",
    "#                                     datasets[\"val_pt\"],\n",
    "#                                     datasets[\"val_ru\"]])\n",
    "\n",
    "# test_dataset = concatenate_datasets([datasets[\"test_de\"], \n",
    "#                                     datasets[\"test_en\"], \n",
    "#                                     datasets[\"test_es\"], \n",
    "#                                     datasets[\"test_fr\"],\n",
    "#                                     datasets[\"test_it\"],\n",
    "#                                     datasets[\"test_nl\"],\n",
    "#                                     datasets[\"test_pl\"],\n",
    "#                                     datasets[\"test_pt\"],\n",
    "#                                     datasets[\"test_ru\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a Transformers Tokenizer which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure:\n",
    "\n",
    " - we get a tokenizer that corresponds to the model architecture we want to use,\n",
    " - we download the vocabulary used when pretraining this specific checkpoint.\n",
    " \n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#the tokenizer below is the checkpoint \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") ## We use the BERT tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following assertion ensures that our tokenizer is a fast tokenizers (backed by Rust) from the Tokenizers library. Those fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly call this tokenizer on one sentence, or you can use the tokenize method to tokenize multiple sentences at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8667, 117, 1142, 1110, 1141, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this is one sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the above output,\n",
    "\n",
    "token_type_ids,\n",
    "\n",
    "is used to distinguish between two sequences that are concatenated together. For example, if we have two sentences \"A cat is walking\" and \"The dog is barking\", and we concatenate them as input to a model, then the tokenizer will assign a different token_type_id to the tokens in the first sentence (e.g. 0) and to the tokens in the second sentence (e.g. 1). In the output of the tokenizer, each token is assigned a token_type_id which indicates which sequence it belongs to.\n",
    "\n",
    "attention_mask,\n",
    "\n",
    "is used to tell the model which tokens to attend to and which ones to ignore. It is a binary vector that has the same length as the input sequence, where each element is either 0 or 1. The 1s indicate the positions of the tokens that should be attended to by the model, and the 0s indicate the positions of the padding tokens that should be ignored. \n",
    "\n",
    "\n",
    "This is a good overal tutorial for preprocessing for Transformer. Check this out to learn more about the preprocessing steps.\n",
    "https://huggingface.co/transformers/preprocessing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the labels of all special tokens to -100 (the index that is ignored by PyTorch) and the labels of all other tokens to the label of the word they come from. Another strategy is to set the label only on the first token obtained from a given word, and give a label of -100 to the other subtokens from the same word. Just change the value of the following flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_all_tokens = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to write the function that will preprocess our samples. We feed them to the tokenizer with the argument truncation=True (to truncate texts that are bigger than the maximum size allowed by the model) and is_split_into_words=True (as seen above). Then we align the labels with the token ids using the strategy we picked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens(i.e.[CLS],[SEP]) have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the map method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\usind\\.cache\\huggingface\\datasets\\Babelscape___parquet\\Babelscape--wikineural-579d1dc98d2a6b93\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-06c6c8d07cfbe611.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2957df7e71394cd1825068b4ab0eaf97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\usind\\.cache\\huggingface\\datasets\\Babelscape___parquet\\Babelscape--wikineural-579d1dc98d2a6b93\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-5948abf95d6f51e4.arrow\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "val_tokenized = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, the results are automatically cached by the Datasets library to avoid spending time on this step the next time you run your notebook. The Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. Datasets warns you when it uses cached files, you can pass load_from_cache_file=False in the call to map to not use the cached files and force the preprocessing to be applied again.\n",
    "\n",
    "Note that we passed batched=True to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label_list), label2id=labels_vocab, id2label=labels_vocab_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning is telling us we are throwing away some weights (the vocab_transform and vocab_layer_norm layers) and randomly initializing some other (the pre_classifier and classifier layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.\n",
    "\n",
    "To instantiate a Trainer, we will need to define three more things. The most important is the TrainingArguments, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-multilingual-cased\" ## You Change this to any generic language model from HuggingFace which supports fine-tuning.\n",
    "args = TrainingArguments(                   ## This is where all your hyperparameters are, reduce *_batch_size if you are working with a GPU which has limited memory.\n",
    "    \"wikineural-multilingual-ner\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,#you can change the batch size according to the computational resources\n",
    "    num_train_epochs=1,   ## may not be enough for a large data, increase this to 5 later in your experiments.\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    \n",
    "    \n",
    "    #learning_rate is a hyperparameter that controls how quickly the model adapts to the training data. It specifies the size of the step taken at each iteration of the training process in the direction of minimizing the loss. A smaller learning rate results in slower but more precise convergence, while a larger learning rate can lead to faster convergence, but may result in overshooting the optimal solution.\n",
    "    #per_device_train_batch_size :This parameter specifies how many training examples should be processed together in one forward/backward pass.\n",
    "    #weight_decay: A regularization term added to the loss function to prevent overfitting. It controls the amount of regularization applied to the weights of the model during training.\n",
    "    #push_to_hub: A flag that specifies whether to push the trained model to the HuggingFace model hub after training. The model hub is a repository of pre-trained models that can be easily shared and used by the community.\n",
    "    #eval_steps: The number of training steps between each evaluation of the model on the validation set. This parameter is used to speed up the training process by evaluating the model less frequently.\n",
    "    #num_train_epochs: The number of times to iterate over the entire training set during training. One epoch means one pass over the entire dataset. Increasing this value may improve the performance of the model, but also increases the training time.\n",
    "\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate,customize the batch_size and the number of epochs for training, as well as the weight decay.\n",
    "\n",
    "Then we will need a data collator that will batch our processed examples together while applying padding to make them all the same size (each pad will be padded to the length of its longest example). There is a data collator for this task in the Transformers library, that not only pads the inputs, but also the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to define for our Trainer is how to compute the metrics from the predictions. Here we will load the seqeval metric (which is commonly used to evaluate results on the CONLL dataset) via the Datasets library.seqeval is a Python library for sequence labeling evaluation. It provides metrics such as precision, recall, and F1-score for named entity recognition (NER) and other sequence labeling tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usind\\AppData\\Local\\Temp\\ipykernel_23804\\152412463.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will need to do a bit of post-processing on our predictions:\n",
    "\n",
    " - select the predicted index (with the maximum logit) for each token\n",
    " - convert it to its string label\n",
    " - ignore everywhere we set a label of -100\n",
    " \n",
    "The following function does all this post-processing on the result of Trainer.evaluate (which is a namedtuple containing predictions and labels) before applying the metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you go any further:\n",
    "1. Login to the HuggigFace CLI on your system using command ``huggingface-cli login``\n",
    "2. Get your access token from your user account on HuggingFace at https://huggingface.co/settings/tokens (preferably a write access token)\n",
    "3. Use this token with the command above to ensure being logged in to HuggingFace.\n",
    "\n",
    "IF you are not working on your own machine with a GPU and have opened this notebook in Colab / Studio Lab, you can use the also cell below to login. You could have used this cell instead of commandline (cli) login, but where is the fun in that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7034e68e024475ca39af133e4af7720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we drop the precision/recall/f1 computed for each category and only focus on the overall precision/recall/f1/accuracy.\n",
    "\n",
    "Then we just need to pass all of this along with our datasets to the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\usind\\anaconda3\\envs\\nlp2022\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 92720\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 46360\n",
      "  Number of trainable parameters = 107726601\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30001' max='46360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30001/46360 1:47:58 < 58:52, 4.63 it/s, Epoch 0.65/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.066523</td>\n",
       "      <td>0.829533</td>\n",
       "      <td>0.881982</td>\n",
       "      <td>0.854954</td>\n",
       "      <td>0.982942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.065096</td>\n",
       "      <td>0.850093</td>\n",
       "      <td>0.883405</td>\n",
       "      <td>0.866429</td>\n",
       "      <td>0.983725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.043984</td>\n",
       "      <td>0.888711</td>\n",
       "      <td>0.901855</td>\n",
       "      <td>0.895235</td>\n",
       "      <td>0.987794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.889353</td>\n",
       "      <td>0.909377</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.988220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.046552</td>\n",
       "      <td>0.885490</td>\n",
       "      <td>0.906328</td>\n",
       "      <td>0.895788</td>\n",
       "      <td>0.987760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.044335</td>\n",
       "      <td>0.882617</td>\n",
       "      <td>0.908412</td>\n",
       "      <td>0.895329</td>\n",
       "      <td>0.988022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.050974</td>\n",
       "      <td>0.884999</td>\n",
       "      <td>0.903126</td>\n",
       "      <td>0.893970</td>\n",
       "      <td>0.987853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.052967</td>\n",
       "      <td>0.886636</td>\n",
       "      <td>0.906734</td>\n",
       "      <td>0.896573</td>\n",
       "      <td>0.987487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.043766</td>\n",
       "      <td>0.897658</td>\n",
       "      <td>0.904091</td>\n",
       "      <td>0.900863</td>\n",
       "      <td>0.988699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.043363</td>\n",
       "      <td>0.911632</td>\n",
       "      <td>0.906582</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.989104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.042800</td>\n",
       "      <td>0.039222</td>\n",
       "      <td>0.902126</td>\n",
       "      <td>0.912122</td>\n",
       "      <td>0.907097</td>\n",
       "      <td>0.989167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.042842</td>\n",
       "      <td>0.908828</td>\n",
       "      <td>0.911970</td>\n",
       "      <td>0.910396</td>\n",
       "      <td>0.989340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.040585</td>\n",
       "      <td>0.911894</td>\n",
       "      <td>0.906379</td>\n",
       "      <td>0.909128</td>\n",
       "      <td>0.989347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.041110</td>\n",
       "      <td>0.911190</td>\n",
       "      <td>0.916747</td>\n",
       "      <td>0.913960</td>\n",
       "      <td>0.989448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.044479</td>\n",
       "      <td>0.904774</td>\n",
       "      <td>0.913189</td>\n",
       "      <td>0.908962</td>\n",
       "      <td>0.988924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.037785</td>\n",
       "      <td>0.916045</td>\n",
       "      <td>0.917814</td>\n",
       "      <td>0.916929</td>\n",
       "      <td>0.990043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.040347</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>0.922186</td>\n",
       "      <td>0.911919</td>\n",
       "      <td>0.989426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.038905</td>\n",
       "      <td>0.912890</td>\n",
       "      <td>0.920407</td>\n",
       "      <td>0.916633</td>\n",
       "      <td>0.990025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.039057</td>\n",
       "      <td>0.910149</td>\n",
       "      <td>0.917967</td>\n",
       "      <td>0.914041</td>\n",
       "      <td>0.989628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>0.913960</td>\n",
       "      <td>0.925388</td>\n",
       "      <td>0.919638</td>\n",
       "      <td>0.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.035960</td>\n",
       "      <td>0.916206</td>\n",
       "      <td>0.925845</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.990522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.037553</td>\n",
       "      <td>0.914974</td>\n",
       "      <td>0.920508</td>\n",
       "      <td>0.917733</td>\n",
       "      <td>0.990028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.034772</td>\n",
       "      <td>0.922208</td>\n",
       "      <td>0.920661</td>\n",
       "      <td>0.921433</td>\n",
       "      <td>0.990661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.033773</td>\n",
       "      <td>0.916120</td>\n",
       "      <td>0.922592</td>\n",
       "      <td>0.919345</td>\n",
       "      <td>0.990623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.036769</td>\n",
       "      <td>0.908567</td>\n",
       "      <td>0.921728</td>\n",
       "      <td>0.915100</td>\n",
       "      <td>0.990103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.034396</td>\n",
       "      <td>0.921918</td>\n",
       "      <td>0.923558</td>\n",
       "      <td>0.922737</td>\n",
       "      <td>0.990848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.034370</td>\n",
       "      <td>0.918606</td>\n",
       "      <td>0.924676</td>\n",
       "      <td>0.921631</td>\n",
       "      <td>0.990695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.031357</td>\n",
       "      <td>0.923381</td>\n",
       "      <td>0.926760</td>\n",
       "      <td>0.925067</td>\n",
       "      <td>0.991121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.033135</td>\n",
       "      <td>0.922866</td>\n",
       "      <td>0.925540</td>\n",
       "      <td>0.924201</td>\n",
       "      <td>0.990687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4396' max='11597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4396/11597 00:46 < 01:15, 94.89 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-1000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-1000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-1000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-2000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-2000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-2000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-3000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-3000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-3000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-4000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-4000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-4000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-5000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-5000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-5000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-6000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-6000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-6000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-7000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-7000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-7000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-8000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-8000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-8000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-9000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-9000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-9000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-10000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-10000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-10000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-11000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-11000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-11000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-12000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-12000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-12000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-13000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-13000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-13000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-13000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-13000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-14000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-14000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-14000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-14000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-14000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-15000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-15000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-15000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-16000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-16000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-16000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-17000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-17000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-17000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-17000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-17000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-18000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-18000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-18000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-18000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-18000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-19000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-19000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-19000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-19000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-19000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-20000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-20000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-20000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-21000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-21000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-21000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-21000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-21000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-22000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-22000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-22000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-22000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-22000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-23000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-23000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-23000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-23000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-23000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-24000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-24000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-24000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-24000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-24000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-25000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-25000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-25000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-25000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-25000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-26000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-26000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-26000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-26000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-26000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-27000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-27000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-27000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-27000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-27000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-28000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-28000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-28000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-28000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-28000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to wikineural-multilingual-ner\\checkpoint-29000\n",
      "Configuration saved in wikineural-multilingual-ner\\checkpoint-29000\\config.json\n",
      "Model weights saved in wikineural-multilingual-ner\\checkpoint-29000\\pytorch_model.bin\n",
      "tokenizer config file saved in wikineural-multilingual-ner\\checkpoint-29000\\tokenizer_config.json\n",
      "Special tokens file saved in wikineural-multilingual-ner\\checkpoint-29000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lang, ner_tags, tokens. If lang, ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 1\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate method allows you to evaluate again on the evaluation dataset or on another dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the precision/recall/f1 computed for each category now that we have finished training, we can apply the same function as before on the result of the predict method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(test_tokenized)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you try to evaluate the NER task performance on a multilingual dataset? with a different language model?\n",
    "1. Think about what Language Model on HuggingFace can help you Fine-tune over a multilingual dataset. Hint: XLM-R\n",
    "2. Go back to the commented out multilingual dataset concatetation notebook cell and try to include a few more languages in the dataset.\n",
    "3. Try to fine-tune the model on this dataset.\n",
    "4. Report the performance in a MARKDOWN CELL below, showing your work on multilingual model fine-tuning in the cells below. \n",
    "5. Submit this notebook to Google Classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a2c79697ca2bf0eeb40b71e65aea15190668944c6faa8d6cdd0a8b2e82a887b"
  },
  "kernelspec": {
   "display_name": "nlp2022",
   "language": "python",
   "name": "nlp2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
